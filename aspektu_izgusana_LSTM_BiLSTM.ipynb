{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### DATU SAGATAVOŠANA"
      ],
      "metadata": {
        "id": "0us6EvDgECVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BrambleXu. aspect term extraction. Tiešsaiste. GitHub: 11.09.2018. Pieejams: https://github.com/BrambleXu/aspect-term-extraction/tree/master. [skatīts 2025-03-23].\n",
        "\n",
        "def soup2dict(sentence_nodes):\n",
        "    \"\"\"\n",
        "    Input: a soup object, e.g. soup.find_all(\"sentence\")\n",
        "    Output: a list of dictionaries, contains id, text, aspect terms\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    i = 0\n",
        "    for n in sentence_nodes:\n",
        "        i += 1\n",
        "        sentence = {}\n",
        "        aspect_term = []\n",
        "        sentence['id'] = i\n",
        "        sentence['text'] = n.find('text').string\n",
        "        if n.find('Opinions'):\n",
        "            for c in n.find('Opinions').contents:\n",
        "                if c.name == 'Opinion':\n",
        "                    if c['target'] not in aspect_term:\n",
        "                        aspect_term.append(c['target'])\n",
        "\n",
        "        sentence['aspect'] = aspect_term\n",
        "        sentences.append(sentence)\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "MBtLaVJ-D8yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split2words(s_text):\n",
        "    \"\"\"Split string with white and prereserve the punctuation\n",
        "    Input:\n",
        "        s_text: string, a sentence, e.g. Judging from previous posts this used to be a good place, but not any longer.\n",
        "    Output:\n",
        "        words: a list of words, e.g. ['judging', 'from', 'previous', 'posts', 'this', 'used', 'to', 'be', 'a', 'good', 'place', ',', 'but', 'not', 'any', 'longer', '.']\n",
        "    \"\"\"\n",
        "    s_text = re.sub('([.,!?()])', r' \\1 ', s_text)\n",
        "    s_text = re.sub('\\s{2,}', ' ', s_text)\n",
        "    words = s_text.lower().split()\n",
        "    return words"
      ],
      "metadata": {
        "id": "vDhZ1iJ3EGqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tagging_IOB(s, aspects):\n",
        "    \"\"\"Assigning IOB tag to each word in s\n",
        "    Input:\n",
        "        s: sentences, a list of words, e.g. ['judging', 'from', 'previous', 'posts']\n",
        "        aspects: a list of aspect term, e.g. ['a good place', 'Posts']\n",
        "    Output:\n",
        "        tag: a list of tag, e.g. ['O', 'O', 'O', 'B']\n",
        "    \"\"\"\n",
        "    tags = ['O'] * len(s)\n",
        "\n",
        "    for aspect in aspects:\n",
        "        pre_index = 0\n",
        "        for word in s:\n",
        "            if word in aspect: # 'good' in 'a good place'\n",
        "                cur_index = s.index(word)\n",
        "                if cur_index - pre_index == 1: # inside an aspect term\n",
        "                    tags[cur_index] = 'I'\n",
        "                else:                       # beginning of an aspect term\n",
        "                    tags[cur_index] = 'B'\n",
        "                pre_index = cur_index\n",
        "    return tags"
      ],
      "metadata": {
        "id": "eHeR8eJbEI1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dict2df(sentences):\n",
        "    \"\"\"Convert list of dict to dataframe\n",
        "    Input:\n",
        "        sentences: a list of dictionaries, contains id, text, aspect terms. The output of raw2dict\n",
        "    Output:\n",
        "        data: a dataframe with three columns, sentence id, words, tag with IOB format\n",
        "    \"\"\"\n",
        "    data_frames = []\n",
        "    data = pd.DataFrame()\n",
        "    for s in sentences:\n",
        "        sentence = {}\n",
        "        sentence['Sentence #'] = s['id']\n",
        "        sentence['Word'] = split2words(s['text'])  # split text to words\n",
        "        s_length = len(sentence['Word']) # the length of sentence, used to generate tag\n",
        "        if len(s['aspect'])==0: # tagging: if no aspect term\n",
        "            sentence['Tag'] = ['O'] * s_length\n",
        "        else:                                               # IOB format tag if aspect exist\n",
        "            aspect_terms = [x.lower() for x in s['aspect']]\n",
        "            sentence['Tag'] = tagging_IOB(sentence['Word'], aspect_terms)\n",
        "\n",
        "        sentence_df = pd.DataFrame(sentence)\n",
        "        data_frames.append(sentence_df)\n",
        "\n",
        "    data = pd.concat(data_frames, ignore_index=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "ISP5rlNvEKXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file_path):\n",
        "    # 1 raw data to soup\n",
        "    soup = None\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        soup = BeautifulSoup(f.read().strip(), \"lxml-xml\")\n",
        "    if soup is None:\n",
        "        raise Exception(\"Can't read xml file\")\n",
        "    sentence_nodes = soup.find_all(\"sentence\")\n",
        "\n",
        "    # 2  convert soup object to a list of dictionaries\n",
        "    sentences = soup2dict(sentence_nodes)\n",
        "\n",
        "    # 3 list to dataframe\n",
        "    data = dict2df(sentences)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "N80b3w3dEOEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from bs4 import BeautifulSoup\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: izmaini šo koda gabalu tā, lai tas darbotos google colab vidē https://chatgpt.com/ [izmantots 2025-03-23]\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "data = read_data(file_name)\n",
        "data.tail()"
      ],
      "metadata": {
        "id": "Oz-xOJ2UEPVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_file = Path('train.csv')\n",
        "data.to_csv(save_file, index=False)"
      ],
      "metadata": {
        "id": "D1cq4F4LERFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM un BILSTM apmācīšana un pārbaudīšana"
      ],
      "metadata": {
        "id": "zMl0k2VfD9lT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIDTwD5TIOf4"
      },
      "outputs": [],
      "source": [
        "# BrambleXu. aspect term extraction. Tiešsaiste. GitHub: 11.09.2018. Pieejams: https://github.com/BrambleXu/aspect-term-extraction/tree/master. [skatīts 2025-03-23].\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from pathlib import Path\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "YLxpRl73IYRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_train\n",
        "# Save all words as a list\n",
        "words = list(set(data['Word'].values))\n",
        "n_words = len(words)\n",
        "\n",
        "tags = list(set(data[\"Tag\"].values))\n",
        "n_tags = len(tags)\n",
        "\n",
        "max_len = 75\n",
        "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
        "n_words = len(word2idx)\n",
        "# word2idx['<unk>'] = len(word2idx) + 1\n",
        "tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2idx['<pad>'] = 0\n",
        "n_tags = len(tag2idx) # Due to <pad>, here total tag number is from 17 to 18"
      ],
      "metadata": {
        "id": "yyzy7EY0IcPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence class\n",
        "class SentenceGetter(object):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
        "#                                                            s[\"POS\"].values.tolist(),\n",
        "                                                           s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "\n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "getter = SentenceGetter(data)\n",
        "sentences = getter.sentences # get all sentences\n",
        "\n",
        "# Word2inx & Padding for X\n",
        "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
        "X_train = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0)\n",
        "\n",
        "# Word2inx & Padding for y\n",
        "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=0)\n",
        "\n",
        "# Get one-hot labels\n",
        "y_train = [to_categorical(i, num_classes=n_tags) for i in y]\n",
        "\n",
        "print(tag2idx)\n",
        "print(len(X_train))\n",
        "print(len(y_train))"
      ],
      "metadata": {
        "id": "lxJR0gLrIdF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: ievēro iepriekš sūtīto ziņu ar ieteikumiem, lai ieviestu kodā izmaiņas, kur netiek ņemti vērā True Negatives jeb 'O'\n",
        "# un kur Precision un Recall tiek rēķināti tikai B un I klasēm, un to pašu principu ievies arī zudumu aprēķinu apmācības procesā veic izmaiņas tieši manis dotajā\n",
        "# kodā https://chatgpt.com/ [izmantots 2025-04-01]\n",
        "def custom_loss(y_true, y_pred):\n",
        "    o_index = tag2idx.get(\"O\")\n",
        "    if o_index is None:\n",
        "        raise ValueError(\"O label not found in tag2idx.\")\n",
        "    mask = tf.cast(tf.not_equal(tf.argmax(y_true, axis=-1), o_index), dtype=tf.float32)\n",
        "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "    loss = loss * mask\n",
        "    return tf.reduce_sum(loss) / (tf.reduce_sum(mask) + 1e-7)\n"
      ],
      "metadata": {
        "id": "JFt_Y3TnIf_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: kā var ieviest manā kodā klašu svarus? https://chatgpt.com/ [izmantots 2025-03-26]\n",
        "y_train = np.array(y_train)\n",
        "y_train_int = np.argmax(y_train, axis=-1)\n",
        "class_weight_dict_idx = {\n",
        "    tag2idx['O']: 1.0,\n",
        "    tag2idx['I']: 10.0,\n",
        "    tag2idx['B']: 10.0\n",
        "}\n",
        "print(tag2idx)\n",
        "\n",
        "import numpy as np\n",
        "sample_weight = np.ones((y_train_int.shape[0], y_train_int.shape[1]), dtype='float32')\n",
        "for i in range(y_train_int.shape[0]):\n",
        "    for j in range(y_train_int.shape[1]):\n",
        "        cls_idx = y_train_int[i,j]\n",
        "        if cls_idx in class_weight_dict_idx:\n",
        "            sample_weight[i,j] = class_weight_dict_idx[cls_idx]\n",
        "        else:\n",
        "            sample_weight[i,j] = 0.0\n"
      ],
      "metadata": {
        "id": "XBdHs3lFIlZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GeeksforGeeks. Long short-term memory (LSTM) RNN in Tensorflow. Tiešsaiste. GeeksforGeeks: 25.02.2025. Pieejams: https://www.geeksforgeeks.org/long-short-term-memory-lstm-rnn-in-tensorflow/. [skatīts 2025-03-23].\n",
        "\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: par pamatu ņēmu kodu no šī avota (https://github.com/BrambleXu/aspect-term-extraction/blob/master/notebooks/bi-lstm-crf.ipynb).\n",
        "# ko vari ieteikt, lai varētu uzlabot manu kodu? jo modelis arī apmācās ļoti lēni, salīdzinājumā ar github repositorija autora rezultātiem, un metriku rezultāti manā\n",
        "# gadījumā arī ir zemi https://chatgpt.com/ [izmantots 2025-03-24]\n",
        "input_layer = Input(shape=(max_len,))\n",
        "model = Embedding(input_dim=len(word2idx) + 1,\n",
        "                  output_dim=200,\n",
        "                  input_length=max_len,\n",
        "                  mask_zero=True,\n",
        "                  trainable=True)(input_layer)\n",
        "model = LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)(model)\n",
        "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)\n",
        "model = Model(input_layer, out)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=custom_loss,\n",
        "              metrics=[\"accuracy\"], run_eagerly=True)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "kDWDVYecIo6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: tagad nepieciešams sagatavot kodu ar BiLSTM algoritmu, es tev iedošu savu tagadējo kodu ar tagadējiem rezultātiem, nepieciešamības\n",
        "# pēc veic labojumus un sniedz ieteikumus https://chatgpt.com/ [izmantots 2025-03-27]\n",
        "input_layer = Input(shape=(max_len,))\n",
        "model = Embedding(input_dim=len(word2idx) + 1,\n",
        "                  output_dim=300,\n",
        "                  input_length=max_len,\n",
        "                  mask_zero=True)(input_layer)\n",
        "model = Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1))(model)\n",
        "model = Dropout(0.2)(model)\n",
        "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)\n",
        "\n",
        "model = Model(input_layer, out)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=custom_loss,\n",
        "              metrics=[\"accuracy\"], run_eagerly=True)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "0YrsLJHKDaYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, np.array(y_train), batch_size=256, epochs=12,\n",
        "                    validation_split=0.1, sample_weight=sample_weight, verbose=1)\n",
        "hist = pd.DataFrame(history.history)\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vk0pcLzdIrS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODEĻU PĀRBAUDĪŠANA"
      ],
      "metadata": {
        "id": "PAeY4I-6GaSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_test\n",
        "getter = SentenceGetter(data)\n",
        "sentences = getter.sentences # get all sentences\n",
        "\n",
        "# Word2inx & Padding for X\n",
        "X = [[word2idx.get(w[0], 0) for w in s] for s in sentences]\n",
        "X_test = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0)\n",
        "\n",
        "# Word2inx & Padding for y\n",
        "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx['<pad>'])\n",
        "\n",
        "# Get one-hot labels\n",
        "y_test = [to_categorical(i, num_classes=n_tags) for i in y]"
      ],
      "metadata": {
        "id": "XEq1gUjyJXvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[1])\n",
        "print(X_test[1])\n",
        "print(np.argmax(y_test[1], -1))"
      ],
      "metadata": {
        "id": "s_gbh_i2Jctx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions.\n",
        "idx2word = {value: key for key, value in word2idx.items()}\n",
        "idx2tag = {value: key for key, value in tag2idx.items()}\n",
        "print(idx2tag)"
      ],
      "metadata": {
        "id": "y9pK7k-hJeEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_all = np.argmax(y_test, -1)\n",
        "true_all_tags = [[idx2tag[idx] for idx in s if idx!=0] for s in true_all]\n",
        "\n",
        "p_all = model.predict(np.array(X_test)) # (4796, 75, 18)\n",
        "p_all= np.argmax(p_all, axis=-1) # (4796, 75)\n",
        "p_all_tags = [[idx2tag[idx] for idx in s] for s in p_all] # ['B-gpe', 'O', 'O', 'O']\n",
        "\n",
        "for i, true in enumerate(true_all_tags):\n",
        "    length = len(true)\n",
        "    p_all_tags[i] = p_all_tags[i][:length]\n",
        "\n",
        "p_all_tags = [[x.replace('<pad>', 'O') for x in s] for s in p_all_tags]\n",
        "\n",
        "print(sentences[3])\n",
        "print(X_test[3])\n",
        "print(true_all_tags[3])\n",
        "print(p_all_tags[3])"
      ],
      "metadata": {
        "id": "waLjLlTQJfHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: kā man izlabot kļūdu ValueError: max() arg is an empty sequence šajā koda daļā? https://chatgpt.com/ [izmantots 2025-03-26]\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: šo kodu es izmantoju modeļa apmācīšanai ar datu kopu, kas ir paredzēta noskaņojumu noteikšanai aspektiem.\n",
        "# CSV faili sastāv no teikuma numura, vārda un marķiera, kas apzīmē šī vārda noskaņojumu (2 - pozitīvs, 1 - neitrāls, 0 - negatīvs, O - nav noskaņojuma)\n",
        "# https://chatgpt.com/ [izmantots 2025-03-26]\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "true_flat = []\n",
        "pred_flat = []\n",
        "for i in range(len(true_all_tags)):\n",
        "    true_flat.extend(true_all_tags[i])\n",
        "    pred_flat.extend(p_all_tags[i])\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: ievēro iepriekš sūtīto ziņu ar ieteikumiem, lai ieviestu kodā izmaiņas, kur netiek ņemti vērā True Negatives jeb 'O' un\n",
        "# kur Precision un Recall tiek rēķināti tikai B un I klasēm, un to pašu principu ievies arī zudumu aprēķinu apmācības procesā veic izmaiņas tieši manis dotajā\n",
        "# kodā https://chatgpt.com/ [izmantots 2025-04-01]\n",
        "def merge_labels(labels):\n",
        "    return [\"positive\" if label in [\"B\", \"I\"] else \"negative\" for label in labels]\n",
        "\n",
        "true_flat_bin = merge_labels(true_flat)\n",
        "pred_flat_bin = merge_labels(pred_flat)\n",
        "\n",
        "accuracy_score_bin = accuracy_score(true_flat_bin, pred_flat_bin)\n",
        "print(accuracy_score_bin)\n",
        "print(classification_report(true_flat_bin, pred_flat_bin, labels=[\"positive\"]))\n"
      ],
      "metadata": {
        "id": "T6YjdziFJkpV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}