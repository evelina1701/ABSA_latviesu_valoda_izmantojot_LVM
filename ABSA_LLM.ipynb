{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIQBQmyQjTfe"
      },
      "outputs": [],
      "source": [
        "! pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install stanza"
      ],
      "metadata": {
        "id": "H_lBXN2RjitP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ASPEKTU IZGŪŠANA"
      ],
      "metadata": {
        "id": "hikUvzB9jkuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. GPT-4o. Uzvedne: kā sasaistīt zemāk dotos kodus un izlabot manu kodu tā, lai pēc katras uzvednes tiktu izgūts rezultāts kurš uzreiz tiek salīdzināts ar attiecīgo pareizo atbildi, nesalīdzinot ar ID, un tad pareizi tiek aprēķinātas metrikas? https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "\n",
        "# OpenRouter. Sample code and API for DeepSeek V3 0324 (free). Tiešsaiste. OpenRouter. Pieejams: https://openrouter.ai/deepseek/deepseek-chat-v3-0324:free/api. [skatīts 2025-03-29].\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: kā man uzlabot šo kodu, lai tas izmantotu no XML izgūtos teikumus, aspektus un noskaņojumus noskaņojuma noteikšanai aspektiem un vēl novērtētu modeļa veiktspēju? https://chatgpt.com/ [izmantots 2025-03-29]\n",
        "\n",
        "def parse_xml_aspect_extraction(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    data = []\n",
        "    for sentence in root.iter('sentence'):\n",
        "        if \"OutOfScope\" in sentence.attrib and sentence.attrib[\"OutOfScope\"].upper() == \"TRUE\":\n",
        "          continue\n",
        "        text = sentence.find('text').text.strip()\n",
        "        opinions = sentence.find('Opinions')\n",
        "        aspects = []\n",
        "        if opinions is not None:\n",
        "            for opinion in opinions.findall('Opinion'):\n",
        "                target = opinion.get('target')\n",
        "                if target and target.lower() != \"null\":\n",
        "                  aspects.append(target.lower())\n",
        "        data.append({\n",
        "            'id': sentence.get('id'),\n",
        "            'text': text,\n",
        "            'aspects': aspects\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=\"\"\n",
        ")\n",
        "def extract_aspects(sentence_id, sentence):\n",
        "    prompt = (\n",
        "        \"Your task is to do aspect term extraction.\\n\"\n",
        "        \"Given a sentence, extract all explicit aspect terms from a sentence.\\n\"\n",
        "        \"Return the result as a single JSON object with key 'results'.\\n\"\n",
        "        \"If there are no aspect terms in the sentence, return an empty list for 'results'.\\n\"\n",
        "        \"Extract the aspect terms exactly as they appear in the sentence, preserving their original form and grammatical case.\\n\"\n",
        "        \"Do not provide any additional text or explanation, do not guess implicit aspects, do not extract adjectives, verbs or general sentiment expressions.\\n\"\n",
        "        \"Do not include broad or vague terms (e.g., 'cena', 'diena') and always preserve the full noun phrase (e.g., 'ķiploku grauzdiņi', not just 'grauzdiņi')\\n\"\n",
        "        \"Respond ONLY with valid JSON.\\n\"\n",
        "        \"Example format:\\n\"\n",
        "        \"{\\\"results\\\": [\\\"ēdiens\\\", \\\"apkalpošana\\\"]}\\n\"\n",
        "        \"{\\\"results\\\": []}\\n\\n\"\n",
        "        f\"Sentence: \\\"{sentence}\\\"\"\n",
        "    )\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"deepseek/deepseek-chat\"\n",
        "        )\n",
        "        content = completion.choices[0].message.content.strip()\n",
        "        match = re.search(r\"\\{.*\\}\", content, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON in response.\")\n",
        "        parsed = json.loads(match.group(0))\n",
        "        return parsed.get(\"results\", [])\n",
        "\n",
        "# OpenAI. GPT-4o. Uzvedne: varbūt būtu labāk šīs nepareizās atbildes ar exception kļūdām nesaglabāt vispār JSON failā bet kaut kur atsevišķi, jo tad vienkārši JSON failā nebūs kļūdainu rezultātu, kurus nevarēs nolasīt un ar šo rindiņu if sentence_id not in prediction_dict: uzreiz tiks uzskatīta kā kļūda https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] ID {sentence_id} — {e}\")\n",
        "        with open(\"llm_errors_aspects.log\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"{sentence_id} — {e}\\n\")\n",
        "            f.write(f\"Original response: {completion}\\n\\n\")\n",
        "        return None\n",
        "\n",
        "# OpenAI. GPT-4o. Uzvedne: kā izlabot manā kodā metriku aprēķināšanas funkciju, lai tiktu iegūti TP, FP un FN? https://chatgpt.com/ [izmantots 2025-04-26]\n",
        "def evaluate_live(test_data):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    skipped = 0\n",
        "    mismatches = []\n",
        "\n",
        "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Processing\"):\n",
        "\n",
        "        current_TP = 0\n",
        "        current_FP = 0\n",
        "        current_FN = 0\n",
        "\n",
        "        sentence_id = row[\"id\"]\n",
        "        sentence = row[\"text\"]\n",
        "        true_aspects = [a.lower() for a in row[\"aspects\"]]\n",
        "        pred_aspects = extract_aspects(row[\"id\"], row[\"text\"])\n",
        "\n",
        "        if pred_aspects is None:\n",
        "            skipped += 1\n",
        "            FN += len(true_aspects)\n",
        "            continue\n",
        "\n",
        "        pred_aspects = [a.lower() for a in pred_aspects]\n",
        "\n",
        "        true_set = set(true_aspects)\n",
        "        pred_set = set(pred_aspects)\n",
        "\n",
        "        current_TP += len(true_set & pred_set)   # Pareizi atrastie aspekti\n",
        "        current_FP += len(pred_set - true_set)   # Nepareizi atrastie aspekti (kļūdaini izdomāti)\n",
        "        current_FN += len(true_set - pred_set)   # Pazudušie aspekti (neatrastie)\n",
        "\n",
        "        TP += current_TP\n",
        "        FP += current_FP\n",
        "        FN += current_FN\n",
        "\n",
        "        if current_FP > 0 or current_FN > 0:\n",
        "            mismatches.append({\n",
        "                'id': sentence_id,\n",
        "                'text': sentence,\n",
        "                'true_aspects': true_aspects,\n",
        "                'predicted_aspects': pred_aspects,\n",
        "                'FP': list(pred_set - true_set),\n",
        "                'FN': list(true_set - pred_set)\n",
        "            })\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"\\nSalīdzināti {len(test_data)} teikumi (izlaisti {skipped})\")\n",
        "    print(f\"True Positives (TP): {TP}\")\n",
        "    print(f\"False Positives (FP): {FP}\")\n",
        "    print(f\"False Negatives (FN): {FN}\")\n",
        "    print(f\"\\nPrecision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 score:  {f1:.4f}\")\n",
        "\n",
        "    with open(\"errors_fp_fn.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(mismatches, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\nFP un FN kļūdu saraksts saglabāts uz 'errors_fp_fn.json'\")\n",
        "\n",
        "test_data = parse_xml_aspect_extraction(\"testGOLD2016_1.xml\")\n",
        "evaluate_live(test_data)"
      ],
      "metadata": {
        "id": "orGGtaiUjmSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ASPEKTU NOSKAŅOJUMU NOTEIKŠANA"
      ],
      "metadata": {
        "id": "JA27lOwpj99Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenRouter. Sample code and API for DeepSeek V3 0324 (free). Tiešsaiste. OpenRouter. Pieejams: https://openrouter.ai/deepseek/deepseek-chat-v3-0324:free/api. [skatīts 2025-03-29].\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: kā man uzlabot šo kodu, lai tas izmantotu no XML izgūtos teikumus, aspektus un noskaņojumus noskaņojuma noteikšanai aspektiem un vēl novērtētu modeļa veiktspēju? https://chatgpt.com/ [izmantots 2025-03-29]\n",
        "# https://console.groq.com/docs/quickstart\n",
        "# OpenAI. GPT-4o. Uzvedne: kā sasaistīt zemāk dotos kodus un izlabot manu kodu tā, lai pēc katras uzvednes tiktu izgūts rezultāts kurš uzreiz tiek salīdzināts ar attiecīgo pareizo atbildi, nesalīdzinot ar ID, un tad pareizi tiek aprēķinātas metrikas? https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "from itertools import zip_longest\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "def parse_xml_sentiment_pred(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    data = []\n",
        "    for sentence in root.iter('sentence'):\n",
        "        if \"OutOfScope\" in sentence.attrib and sentence.attrib[\"OutOfScope\"].upper() == \"TRUE\":\n",
        "          continue\n",
        "        text = sentence.find('text').text.strip()\n",
        "        opinions = sentence.find('Opinions')\n",
        "        aspects = []\n",
        "        sentiments = []\n",
        "        if opinions is not None:\n",
        "            for opinion in opinions.findall('Opinion'):\n",
        "                target = opinion.get('target')\n",
        "                polarity = opinion.get('polarity')\n",
        "                if target and target.lower() != \"null\":\n",
        "                  aspects.append(target.lower())\n",
        "                  sentiments.append(polarity)\n",
        "        if aspects:\n",
        "          data.append({\n",
        "              'id': sentence.get('id'),\n",
        "              'text': text,\n",
        "              'aspects': aspects,\n",
        "              'sentiments': sentiments\n",
        "          })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=\"\"\n",
        ")\n",
        "def predict_sentiment_only_labels(sentence_id, sentence, aspects):\n",
        "    aspects_str = \", \".join(f'\\\"{asp}\\\"' for asp in aspects)\n",
        "    prompt = (\n",
        "        \"Your task is to perform aspect-based sentiment analysis.\\n\"\n",
        "        \"Classify the sentiment for each of the listed aspect terms in the given sentence.\\n\"\n",
        "        \"Use only one of the following labels: positive, negative, neutral.\\n\"\n",
        "        \"Return a singe JSON object with key 'results' and value as a list of sentiment labels in the same order as the aspects provided.\\n\"\n",
        "        \"Respond ONLY with valid JSON like: {\\\"results\\\": [\\\"positive\\\", \\\"neutral\\\"]}\\n\"\n",
        "        \"Do NOT include any extra text or explanations.\\n\"\n",
        "        f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
        "        f\"Aspect terms: [{aspects_str}]\"\n",
        "    )\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"deepseek/deepseek-chat\"\n",
        "        )\n",
        "        content = completion.choices[0].message.content.strip()\n",
        "        match = re.search(r\"\\{.*\\}\", content, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON in response.\")\n",
        "        parsed = json.loads(match.group(0))\n",
        "        results = parsed.get(\"results\", [])\n",
        "        return results\n",
        "\n",
        "# OpenAI. GPT-4o. Uzvedne: varbūt būtu labāk šīs nepareizās atbildes ar exception kļūdām nesaglabāt vispār JSON failā bet kaut kur atsevišķi, jo tad vienkārši JSON failā nebūs kļūdainu rezultātu, kurus nevarēs nolasīt un ar šo rindiņu if sentence_id not in prediction_dict: uzreiz tiks uzskatīta kā kļūda https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] ID {sentence_id} — {e}\")\n",
        "        with open(\"llm_errors_sentiments.log\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"{sentence_id} — {e}\\n\")\n",
        "            if 'completion' in locals():\n",
        "                f.write(f\"Original response: {completion}\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"Original response: N/A (no response returned)\\n\\n\")\n",
        "        return None\n",
        "# OpenAI. GPT-4o. Uzvedne: kā izlabot manā kodā metriku aprēķināšanas funkciju, lai tiktu iegūti TP, FP un FN? https://chatgpt.com/ [izmantots 2025-04-26]\n",
        "# OpenAI. ChatGPT o4-mini-high. Uzvedne: kā how to fix evaluate_sentiment_classification, so that it checks if pred_sentiments is None, checks if sentiment is valid (positive, negative, neutral), checks if there are missing sentiments and if there are excess sentiments, checks if sentiment is correct and matches true sentiment https://chatgpt.com/ [izmantots 2025-05-07]\n",
        "\n",
        "def evaluate_sentiment_classification(test_data):\n",
        "    TP = FP = FN = 0\n",
        "    errors = []\n",
        "    skipped = 0\n",
        "    valid_labels = {\"positive\", \"negative\", \"neutral\"}\n",
        "\n",
        "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Processing\"):\n",
        "        true_sentiments = [s.lower() for s in row[\"sentiments\"]]\n",
        "        pred_sentiments = predict_sentiment_only_labels(row[\"id\"], row[\"text\"], row[\"aspects\"])\n",
        "\n",
        "        if pred_sentiments is None:\n",
        "            FN += len(true_sentiments)\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        for true_s, pred_s in zip_longest(true_sentiments, pred_sentiments, fillvalue=None):\n",
        "            if pred_s is None:\n",
        "                FN += 1\n",
        "                errors.append({\n",
        "                    \"id\": row[\"id\"],\n",
        "                    \"aspect\": row[\"aspects\"][true_sentiments.index(true_s)],\n",
        "                    \"true_sentiment\": true_s,\n",
        "                    \"predicted_sentiment\": None,\n",
        "                    \"error_type\": \"Missing prediction\"\n",
        "                })\n",
        "\n",
        "            elif true_s is None:\n",
        "                FP += 1\n",
        "                errors.append({\n",
        "                    \"id\": row[\"id\"],\n",
        "                    \"aspect\": \"EXTRA_PREDICTED\",\n",
        "                    \"true_sentiment\": None,\n",
        "                    \"predicted_sentiment\": pred_s,\n",
        "                    \"error_type\": \"Extra prediction\"\n",
        "                })\n",
        "\n",
        "            else:\n",
        "                pred_norm = pred_s.strip().lower()\n",
        "\n",
        "                if pred_norm not in valid_labels:\n",
        "                    FN += 1\n",
        "                    errors.append({\n",
        "                        \"id\": row[\"id\"],\n",
        "                        \"aspect\": row[\"aspects\"][true_sentiments.index(true_s)],\n",
        "                        \"true_sentiment\": true_s,\n",
        "                        \"predicted_sentiment\": pred_s,\n",
        "                        \"error_type\": \"Invalid label\"\n",
        "                    })\n",
        "\n",
        "                elif pred_norm == true_s:\n",
        "                    TP += 1\n",
        "\n",
        "                else:\n",
        "                    FP += 1\n",
        "                    FN += 1\n",
        "                    errors.append({\n",
        "                        \"id\": row[\"id\"],\n",
        "                        \"aspect\": row[\"aspects\"][true_sentiments.index(true_s)],\n",
        "                        \"true_sentiment\": true_s,\n",
        "                        \"predicted_sentiment\": pred_norm,\n",
        "                        \"error_type\": \"Incorrect label\"\n",
        "                    })\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall    = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"\\nSalīdzināti {len(test_data)} teikumi (izlaisti {skipped})\")\n",
        "    print(f\"True Positives (TP): {TP}\")\n",
        "    print(f\"False Positives (FP): {FP}\")\n",
        "    print(f\"False Negatives (FN): {FN}\")\n",
        "    print(f\"\\nPrecision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 score:  {f1:.4f}\")\n",
        "\n",
        "    with open(\"errors_sentiment_labels.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(errors, f, ensure_ascii=False, indent=2)\n",
        "    print(\"\\nFP un FN kļūdu saraksts saglabāts uz errors_sentiment_labels.json\")\n",
        "\n",
        "test_data = parse_xml_sentiment_pred(\"LVtestGOLD_1.xml\")\n",
        "evaluate_sentiment_classification(test_data)\n"
      ],
      "metadata": {
        "id": "ZWOy1961kAIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VIENLAICĪGA ASPEKTU IZGŪŠANA UN NOSKAŅOJUMU NOTEIKŠANA"
      ],
      "metadata": {
        "id": "z5-4jGy5klA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenRouter. Sample code and API for DeepSeek V3 0324 (free). Tiešsaiste. OpenRouter. Pieejams: https://openrouter.ai/deepseek/deepseek-chat-v3-0324:free/api. [skatīts 2025-03-29].\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: kā man uzlabot šo kodu, lai tas izmantotu no XML izgūtos teikumus, aspektus un noskaņojumus noskaņojuma noteikšanai aspektiem un vēl novērtētu modeļa veiktspēju? https://chatgpt.com/ [izmantots 2025-03-29]\n",
        "# OpenAI. GPT-4o. Uzvedne: kā sasaistīt zemāk dotos kodus un izlabot manu kodu tā, lai pēc katras uzvednes tiktu izgūts rezultāts kurš uzreiz tiek salīdzināts ar attiecīgo pareizo atbildi, nesalīdzinot ar ID, un tad pareizi tiek aprēķinātas metrikas? https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "\n",
        "def parse_xml_full_absa(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    data = []\n",
        "    for sentence in root.iter('sentence'):\n",
        "        if \"OutOfScope\" in sentence.attrib and sentence.attrib[\"OutOfScope\"].upper() == \"TRUE\":\n",
        "          continue\n",
        "        text = sentence.find('text').text.strip()\n",
        "        opinions = sentence.find('Opinions')\n",
        "        aspects = []\n",
        "        sentiments = []\n",
        "        if opinions is not None:\n",
        "            for opinion in opinions.findall('Opinion'):\n",
        "                target = opinion.get('target')\n",
        "                polarity = opinion.get('polarity')\n",
        "                if target and target.lower() != \"null\":\n",
        "                  aspects.append(target.lower())\n",
        "                  sentiments.append(polarity)\n",
        "        data.append({\n",
        "            'id': sentence.get('id'),\n",
        "            'text': text,\n",
        "            'aspects': aspects,\n",
        "            'sentiments': sentiments\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=\"\"\n",
        ")\n",
        "def predict_sentiment(sentence_id, sentence):\n",
        "    prompt = (\n",
        "        \"Your task is to do aspect based sentiment analysis.\\n\"\n",
        "        \"Given a sentence, extract all explicit aspect terms from a sentence and determine the sentiment towards each extracted aspect. Use only: positive, negative, neutral.\\n\"\n",
        "        \"Return the result as a JSON object with a single key called 'results', where the value is a list of [aspect, sentiment] pairs.\\n\"\n",
        "        \"Example: \\n\"\n",
        "        \"{\\\"results\\\": [[\\\"ēdiens\\\", \\\"positive\\\"], [\\\"apkalpošana\\\", \\\"neutral\\\"]]}\\n\"\n",
        "        \"If there are no aspect terms in the sentence, return an empty list for 'results'.\\n\"\n",
        "        \"{\\\"results\\\": []}\\n\\n\"\n",
        "        \"Extract the aspect terms exactly as they appear in the sentence, preserving their original form and grammatical case.\\n\"\n",
        "        \"Do not provide any additional text or explanation and do not include implicit aspects or infer anything not explicitly stated.\\n\"\n",
        "        \"Do not include general concepts (e.g., 'cena', 'skaitlis', 'daudzums', 'pieredze').\\n\"\n",
        "        \"If the same aspect appears with mixed sentiment, repeat it for each sentiment (e.g., `baraviku zupa:positive, baraviku zupa:negative`)\\n\"\n",
        "        \"Do not use monetary values or pronouns (e.g., '11€', 'šīs') as aspects.\\n\"\n",
        "        \"Do not include any adjectives, verbs or general sentiment expressions.\\n\"\n",
        "        \"Respond ONLY with valid JSON. Do not add any text before or after.\\n\"\n",
        "        f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
        "    )\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"gemma2-9b-it\"\n",
        "        )\n",
        "        content = completion.choices[0].message.content.strip()\n",
        "        match = re.search(r\"\\{.*\\}\", content, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON in response.\")\n",
        "        parsed = json.loads(match.group(0))\n",
        "        if isinstance(parsed, list):\n",
        "          parsed = parsed[0]\n",
        "        return parsed.get(\"results\", [])\n",
        "\n",
        "# OpenAI. GPT-4o. Uzvedne: varbūt būtu labāk šīs nepareizās atbildes ar exception kļūdām nesaglabāt vispār JSON failā bet kaut kur atsevišķi, jo tad vienkārši JSON failā nebūs kļūdainu rezultātu, kurus nevarēs nolasīt un ar šo rindiņu if sentence_id not in prediction_dict: uzreiz tiks uzskatīta kā kļūda https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] ID {sentence_id} — {e}\")\n",
        "        with open(\"llm_errors_full.log\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"{sentence_id} — {e}\\n\")\n",
        "            f.write(f\"Original response: {completion}\\n\\n\")\n",
        "        return None\n",
        "\n",
        "# OpenAI. GPT-4o. Uzvedne: kā izlabot manā kodā metriku aprēķināšanas funkciju vienlaicīgā aspektu izgūšanā un aspektu noskaņojuma noteikšanā, lai tiktu iegūti TP, FP un FN? aspekti var atkārtoties un tiem varbūt vienādi vai dažādi noskaņojumi https://chatgpt.com/ [izmantots 2025-04-26]\n",
        "from collections import Counter\n",
        "\n",
        "def evaluate_live(test_data):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    skipped = 0\n",
        "    fp_examples = []\n",
        "    fn_examples = []\n",
        "\n",
        "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Processing\"):\n",
        "        true_aspects = [a.lower() for a in row[\"aspects\"]]\n",
        "        true_sentiments = [s.lower() for s in row[\"sentiments\"]]\n",
        "\n",
        "        pred_aspects = predict_sentiment(row[\"id\"], row[\"text\"])\n",
        "\n",
        "        if pred_aspects is None:\n",
        "            skipped += 1\n",
        "            FN += len(true_aspects)\n",
        "            continue\n",
        "\n",
        "        pred_aspect_terms = [a[0].lower() for a in pred_aspects if isinstance(a, list) and len(a) == 2]\n",
        "        pred_sentiments = [a[1].lower() for a in pred_aspects if isinstance(a, list) and len(a) == 2]\n",
        "\n",
        "        true_counter = Counter(zip(true_aspects, true_sentiments))\n",
        "        pred_counter = Counter(zip(pred_aspect_terms, pred_sentiments))\n",
        "\n",
        "        for pair in (true_counter.keys() | pred_counter.keys()):\n",
        "            true_count = true_counter.get(pair, 0)\n",
        "            pred_count = pred_counter.get(pair, 0)\n",
        "\n",
        "            TP += min(true_count, pred_count)\n",
        "            if pred_count > true_count:\n",
        "                FP += pred_count - true_count\n",
        "                fp_examples.append({\n",
        "                    \"id\": row[\"id\"],\n",
        "                    \"text\": row[\"text\"],\n",
        "                    \"wrong_prediction\": pair\n",
        "                })\n",
        "            if true_count > pred_count:\n",
        "                FN += true_count - pred_count\n",
        "                fn_examples.append({\n",
        "                    \"id\": row[\"id\"],\n",
        "                    \"text\": row[\"text\"],\n",
        "                    \"missing\": pair\n",
        "                })\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"\\nSalīdzināti {len(test_data)} teikumi (izlaisti {skipped})\")\n",
        "    print(f\"True Positives (TP): {TP}\")\n",
        "    print(f\"False Positives (FP): {FP}\")\n",
        "    print(f\"False Negatives (FN): {FN}\")\n",
        "    print(f\"\\nPrecision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 score:  {f1:.4f}\")\n",
        "\n",
        "    with open(\"fp_examples.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fp_examples, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    with open(\"fn_examples.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fn_examples, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\nFP un FN piemēri saglabāti 'fp_examples.json' un 'fn_examples.json'!\")\n",
        "\n",
        "test_data = parse_xml_full_absa(\"testGOLD2016_1.xml\")\n",
        "evaluate_live(test_data)"
      ],
      "metadata": {
        "id": "sszz3WSwkq_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VIENLAICĪGA ASPEKTU IZGŪŠANA UN NOSKAŅOJUMU NOTEIKŠANA AR PIEMĒRU"
      ],
      "metadata": {
        "id": "jFVDzA0hkyO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenRouter. Sample code and API for DeepSeek V3 0324 (free). Tiešsaiste. OpenRouter. Pieejams: https://openrouter.ai/deepseek/deepseek-chat-v3-0324:free/api. [skatīts 2025-03-29].\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: kā man uzlabot šo kodu, lai tas izmantotu no XML izgūtos teikumus, aspektus un noskaņojumus noskaņojuma noteikšanai aspektiem un vēl novērtētu modeļa veiktspēju? https://chatgpt.com/ [izmantots 2025-03-29]\n",
        "# OpenAI. GPT-4o. Uzvedne: kā sasaistīt zemāk dotos kodus un izlabot manu kodu tā, lai pēc katras uzvednes tiktu izgūts rezultāts kurš uzreiz tiek salīdzināts ar attiecīgo pareizo atbildi, nesalīdzinot ar ID, un tad pareizi tiek aprēķinātas metrikas? https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "\n",
        "def parse_xml_full_absa(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    data = []\n",
        "    for sentence in root.iter('sentence'):\n",
        "        if \"OutOfScope\" in sentence.attrib and sentence.attrib[\"OutOfScope\"].upper() == \"TRUE\":\n",
        "          continue\n",
        "        text = sentence.find('text').text.strip()\n",
        "        opinions = sentence.find('Opinions')\n",
        "        aspects = []\n",
        "        sentiments = []\n",
        "        if opinions is not None:\n",
        "            for opinion in opinions.findall('Opinion'):\n",
        "                target = opinion.get('target')\n",
        "                polarity = opinion.get('polarity')\n",
        "                if target and target.lower() != \"null\":\n",
        "                  aspects.append(target.lower())\n",
        "                  sentiments.append(polarity)\n",
        "        data.append({\n",
        "            'id': sentence.get('id'),\n",
        "            'text': text,\n",
        "            'aspects': aspects,\n",
        "            'sentiments': sentiments\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=\"\"\n",
        ")\n",
        "\n",
        "def predict_sentiment(sentence_id, sentence):\n",
        "    prompt = (\n",
        "        \"Your task is to do aspect based sentiment analysis.\\n\"\n",
        "        \"Given a sentence, extract all explicit aspect terms from a sentence and determine the sentiment towards each extracted aspect. Use only: positive, negative, neutral.\\n\"\n",
        "        \"Return the result as a JSON object with a single key called 'results', where the value is a list of [aspect, sentiment] pairs.\\n\"\n",
        "        \"Example: \\n\"\n",
        "        \"{\\\"results\\\": [[\\\"ēdiens\\\", \\\"positive\\\"], [\\\"apkalpošana\\\", \\\"neutral\\\"]]}\\n\"\n",
        "        \"If there are no aspect terms in the sentence, return an empty list for 'results'.\\n\"\n",
        "        \"{\\\"results\\\": []}\\n\\n\"\n",
        "        \"Extract the aspect terms exactly as they appear in the sentence, preserving their original form and grammatical case.\\n\"\n",
        "        \"Do not provide any additional text or explanation and do not include implicit aspects or infer anything not explicitly stated.\\n\"\n",
        "        \"Do not include general concepts (e.g., 'cena', 'skaitlis', 'daudzums', 'pieredze').\\n\"\n",
        "        \"If the same aspect appears with mixed sentiment, repeat it for each sentiment (e.g., `baraviku zupa:positive, baraviku zupa:negative`)\\n\"\n",
        "        \"Do not use monetary values or pronouns (e.g., '11€', 'šīs') as aspects.\\n\"\n",
        "        \"Do not include any adjectives, verbs or general sentiment expressions.\\n\"\n",
        "        \"Respond ONLY with valid JSON. Do not add any text before or after.\\n\"\n",
        "        \"Example:\\n\"\n",
        "        \"Sentence: \\\"Ēdiens bija vidējs līdz virs vidējā; franču sīpolu zupa bija sātīga, bet nekas pārsteidzošs, un deserti nebija nevienā ziņā izcili.\\\"\\n\"\n",
        "        \"Answer: {\\\"results\\\": [[\\\"Ēdiens\\\", \\\"positive\\\"], [\\\"franču sīpolu zupa\\\", \\\"negative\\\"], [\\\"franču sīpolu zupa\\\", \\\"positive\\\"], [\\\"deserti\\\", \\\"negative\\\"]]}\\n\\n\"\n",
        "        f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
        "    )\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"gemma2-9b-it\"\n",
        "        )\n",
        "        content = completion.choices[0].message.content.strip()\n",
        "        match = re.search(r\"\\{.*\\}\", content, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON in response.\")\n",
        "        parsed = json.loads(match.group(0))\n",
        "        if isinstance(parsed, list):\n",
        "          parsed = parsed[0]\n",
        "        return parsed.get(\"results\", [])\n",
        "\n",
        "# OpenAI. GPT-4o. Uzvedne: varbūt būtu labāk šīs nepareizās atbildes ar exception kļūdām nesaglabāt vispār JSON failā bet kaut kur atsevišķi, jo tad vienkārši JSON failā nebūs kļūdainu rezultātu, kurus nevarēs nolasīt un ar šo rindiņu if sentence_id not in prediction_dict: uzreiz tiks uzskatīta kā kļūda https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] ID {sentence_id} — {e}\")\n",
        "        with open(\"llm_errors_full1.log\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"{sentence_id} — {e}\\n\")\n",
        "            f.write(f\"Original response: {completion}\\n\\n\")\n",
        "        return None\n",
        "\n",
        "# OpenAI. GPT-4o. Uzvedne: kā izlabot manā kodā metriku aprēķināšanas funkciju vienlaicīgā aspektu izgūšanā un aspektu noskaņojuma noteikšanā, lai tiktu iegūti TP, FP un FN? aspekti var atkārtoties un tiem varbūt vienādi vai dažādi noskaņojumi https://chatgpt.com/ [izmantots 2025-04-26]\n",
        "from collections import Counter\n",
        "\n",
        "def evaluate_live(test_data):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    skipped = 0\n",
        "    fp_examples = []\n",
        "    fn_examples = []\n",
        "\n",
        "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Processing\"):\n",
        "        true_aspects = [a.lower() for a in row[\"aspects\"]]\n",
        "        true_sentiments = [s.lower() for s in row[\"sentiments\"]]\n",
        "\n",
        "        pred_aspects = predict_sentiment(row[\"id\"], row[\"text\"])\n",
        "\n",
        "        if pred_aspects is None:\n",
        "            skipped += 1\n",
        "            FN += len(true_aspects)\n",
        "            continue\n",
        "\n",
        "        pred_aspect_terms = [a[0].lower() for a in pred_aspects if isinstance(a, list) and len(a) == 2]\n",
        "        pred_sentiments = [a[1].lower() for a in pred_aspects if isinstance(a, list) and len(a) == 2]\n",
        "\n",
        "        true_counter = Counter(zip(true_aspects, true_sentiments))\n",
        "        pred_counter = Counter(zip(pred_aspect_terms, pred_sentiments))\n",
        "\n",
        "        for pair in (true_counter.keys() | pred_counter.keys()):\n",
        "            true_count = true_counter.get(pair, 0)\n",
        "            pred_count = pred_counter.get(pair, 0)\n",
        "\n",
        "            TP += min(true_count, pred_count)\n",
        "            if pred_count > true_count:\n",
        "                FP += pred_count - true_count\n",
        "                fp_examples.append({\n",
        "                    \"id\": row[\"id\"],\n",
        "                    \"text\": row[\"text\"],\n",
        "                    \"wrong_prediction\": pair\n",
        "                })\n",
        "            if true_count > pred_count:\n",
        "                FN += true_count - pred_count\n",
        "                fn_examples.append({\n",
        "                    \"id\": row[\"id\"],\n",
        "                    \"text\": row[\"text\"],\n",
        "                    \"missing\": pair\n",
        "                })\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"\\nSalīdzināti {len(test_data)} teikumi (izlaisti {skipped})\")\n",
        "    print(f\"True Positives (TP): {TP}\")\n",
        "    print(f\"False Positives (FP): {FP}\")\n",
        "    print(f\"False Negatives (FN): {FN}\")\n",
        "    print(f\"\\nPrecision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 score:  {f1:.4f}\")\n",
        "\n",
        "    with open(\"fp_examples1.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fp_examples, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    with open(\"fn_examples1.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fn_examples, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\nFP un FN piemēri saglabāti 'fp_examples.json' un 'fn_examples.json'!\")\n",
        "\n",
        "test_data = parse_xml_full_absa(\"LVtestGOLD_1.xml\")\n",
        "evaluate_live(test_data)"
      ],
      "metadata": {
        "id": "fHRLvdfwk3pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ASPEKTU IZGŪŠANA AR LEMMATIZĀCIJU"
      ],
      "metadata": {
        "id": "cGer019SlCSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. GPT-4o. Uzvedne: kā sasaistīt zemāk dotos kodus un izlabot manu kodu tā, lai pēc katras uzvednes tiktu izgūts rezultāts kurš uzreiz tiek salīdzināts ar attiecīgo pareizo atbildi, nesalīdzinot ar ID, un tad pareizi tiek aprēķinātas metrikas? https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "# OpenAI. GPT-4o. Uzvedne: lemmatizāciju tā, lai lemmatizācija aspektiem tiktu veikta pašā pārbaudē, kur tiek skaitīti TP FP FN? https://chatgpt.com/ [izmantots 2025-05-08]\n",
        "\n",
        "import stanza\n",
        "stanza.download(\"lv\")\n",
        "nlp = stanza.Pipeline(\"lv\", processors=\"tokenize,pos,lemma\", use_gpu=False)\n",
        "def lemmatize_phrase(phrase):\n",
        "    doc = nlp(phrase)\n",
        "    return \" \".join([word.lemma.lower() for sent in doc.sentences for word in sent.words])\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "\n",
        "# OpenRouter. Sample code and API for DeepSeek V3 0324 (free). Tiešsaiste. OpenRouter. Pieejams: https://openrouter.ai/deepseek/deepseek-chat-v3-0324:free/api. [skatīts 2025-03-29].\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: kā man uzlabot šo kodu, lai tas izmantotu no XML izgūtos teikumus, aspektus un noskaņojumus noskaņojuma noteikšanai aspektiem un vēl novērtētu modeļa veiktspēju? https://chatgpt.com/ [izmantots 2025-03-29]\n",
        "\n",
        "def parse_xml_aspect_extraction(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    data = []\n",
        "    for sentence in root.iter('sentence'):\n",
        "        if \"OutOfScope\" in sentence.attrib and sentence.attrib[\"OutOfScope\"].upper() == \"TRUE\":\n",
        "          continue\n",
        "        text = sentence.find('text').text.strip()\n",
        "        opinions = sentence.find('Opinions')\n",
        "        aspects = []\n",
        "        if opinions is not None:\n",
        "            for opinion in opinions.findall('Opinion'):\n",
        "                target = opinion.get('target')\n",
        "                if target and target.lower() != \"null\":\n",
        "                  aspects.append(target.lower())\n",
        "        data.append({\n",
        "            'id': sentence.get('id'),\n",
        "            'text': text,\n",
        "            'aspects': aspects\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=\"\"\n",
        ")\n",
        "def extract_aspects(sentence_id, sentence):\n",
        "    prompt = (\n",
        "        \"Your task is to do aspect term extraction.\\n\"\n",
        "        \"Given a sentence, extract all explicit aspect terms from a sentence.\\n\"\n",
        "        \"Return the result as a single JSON object with key 'results'.\\n\"\n",
        "        \"If there are no aspect terms in the sentence, return an empty list for 'results'.\\n\"\n",
        "        \"Extract the aspect terms exactly as they appear in the sentence, preserving their original form and grammatical case.\\n\"\n",
        "        \"Do not provide any additional text or explanation, do not guess implicit aspects, do not extract adjectives, verbs or general sentiment expressions.\\n\"\n",
        "        \"Do not include broad or vague terms (e.g., 'cena', 'diena') and always preserve the full noun phrase (e.g., 'ķiploku grauzdiņi', not just 'grauzdiņi')\\n\"\n",
        "        \"Respond ONLY with valid JSON.\\n\"\n",
        "        \"Example format:\\n\"\n",
        "        \"{\\\"results\\\": [\\\"ēdiens\\\", \\\"apkalpošana\\\"]}\\n\"\n",
        "        \"{\\\"results\\\": []}\\n\\n\"\n",
        "        f\"Sentence: \\\"{sentence}\\\"\"\n",
        "    )\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"qwen-qwq-32b\"\n",
        "        )\n",
        "        content = completion.choices[0].message.content.strip()\n",
        "        match = re.search(r\"\\{.*\\}\", content, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON in response.\")\n",
        "        parsed = json.loads(match.group(0))\n",
        "        return parsed.get(\"results\", [])\n",
        "\n",
        "# OpenAI. GPT-4o. Uzvedne: varbūt būtu labāk šīs nepareizās atbildes ar exception kļūdām nesaglabāt vispār JSON failā bet kaut kur atsevišķi, jo tad vienkārši JSON failā nebūs kļūdainu rezultātu, kurus nevarēs nolasīt un ar šo rindiņu if sentence_id not in prediction_dict: uzreiz tiks uzskatīta kā kļūda https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] ID {sentence_id} — {e}\")\n",
        "        with open(\"llm_errors_aspects.log\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"{sentence_id} — {e}\\n\")\n",
        "            f.write(f\"Original response: {completion}\\n\\n\")\n",
        "        return None\n",
        "\n",
        "# OpenAI. GPT-4o. Uzvedne: kā izlabot manā kodā metriku aprēķināšanas funkciju, lai tiktu iegūti TP, FP un FN? https://chatgpt.com/ [izmantots 2025-04-26]\n",
        "\n",
        "def evaluate_live(test_data):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    skipped = 0\n",
        "    mismatches = []\n",
        "\n",
        "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Processing\"):\n",
        "\n",
        "        current_TP = 0\n",
        "        current_FP = 0\n",
        "        current_FN = 0\n",
        "\n",
        "        sentence_id = row[\"id\"]\n",
        "        sentence = row[\"text\"]\n",
        "        true_aspects = [lemmatize_phrase(a.lower()) for a in row[\"aspects\"]]\n",
        "        pred_aspects = extract_aspects(row[\"id\"], row[\"text\"])\n",
        "\n",
        "\n",
        "        if pred_aspects is None:\n",
        "            skipped += 1\n",
        "            FN += len(true_aspects)\n",
        "            continue\n",
        "\n",
        "        pred_aspects = [lemmatize_phrase(a.lower()) for a in pred_aspects]\n",
        "\n",
        "        true_set = set(true_aspects)\n",
        "        pred_set = set(pred_aspects)\n",
        "\n",
        "        current_TP += len(true_set & pred_set)   # Pareizi atrastie aspekti\n",
        "        current_FP += len(pred_set - true_set)   # Nepareizi atrastie aspekti (kļūdaini izdomāti)\n",
        "        current_FN += len(true_set - pred_set)   # Pazudušie aspekti (neatrastie)\n",
        "\n",
        "        TP += current_TP\n",
        "        FP += current_FP\n",
        "        FN += current_FN\n",
        "\n",
        "        if current_FP > 0 or current_FN > 0:\n",
        "            mismatches.append({\n",
        "                'id': sentence_id,\n",
        "                'text': sentence,\n",
        "                'true_aspects': true_aspects,\n",
        "                'predicted_aspects': pred_aspects,\n",
        "                'FP': list(pred_set - true_set),\n",
        "                'FN': list(true_set - pred_set)\n",
        "            })\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"\\nSalīdzināti {len(test_data)} teikumi (izlaisti {skipped})\")\n",
        "    print(f\"True Positives (TP): {TP}\")\n",
        "    print(f\"False Positives (FP): {FP}\")\n",
        "    print(f\"False Negatives (FN): {FN}\")\n",
        "    print(f\"\\nPrecision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 score:  {f1:.4f}\")\n",
        "\n",
        "    with open(\"errors_fp_fn.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(mismatches, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\nFP un FN kļūdu saraksts saglabāts uz 'errors_fp_fn.json'\")\n",
        "\n",
        "test_data = parse_xml_aspect_extraction(\"testGOLD2016_1.xml\")\n",
        "evaluate_live(test_data)"
      ],
      "metadata": {
        "id": "8DfBjwH3lFD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KOMBINĒTAIS UZDEVUMS AR LEMMATIZĀCIJU"
      ],
      "metadata": {
        "id": "T6siGNpKlM27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenRouter. Sample code and API for DeepSeek V3 0324 (free). Tiešsaiste. OpenRouter. Pieejams: https://openrouter.ai/deepseek/deepseek-chat-v3-0324:free/api. [skatīts 2025-03-29].\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: kā man uzlabot šo kodu, lai tas izmantotu no XML izgūtos teikumus, aspektus un noskaņojumus noskaņojuma noteikšanai aspektiem un vēl novērtētu modeļa veiktspēju? https://chatgpt.com/ [izmantots 2025-03-29]\n",
        "# OpenAI. GPT-4o. Uzvedne: kā sasaistīt zemāk dotos kodus un izlabot manu kodu tā, lai pēc katras uzvednes tiktu izgūts rezultāts kurš uzreiz tiek salīdzināts ar attiecīgo pareizo atbildi, nesalīdzinot ar ID, un tad pareizi tiek aprēķinātas metrikas? https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "# OpenAI. GPT-4o. Uzvedne: lemmatizāciju tā, lai lemmatizācija aspektiem tiktu veikta pašā pārbaudē, kur tiek skaitīti TP FP FN? https://chatgpt.com/ [izmantots 2025-05-08]\n",
        "\n",
        "import stanza\n",
        "stanza.download(\"lv\")\n",
        "nlp = stanza.Pipeline(\"lv\", processors=\"tokenize,pos,lemma\", use_gpu=False)\n",
        "def lemmatize_phrase(phrase):\n",
        "    doc = nlp(phrase)\n",
        "    return \" \".join([word.lemma.lower() for sent in doc.sentences for word in sent.words])\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "\n",
        "def parse_xml_full_absa(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    data = []\n",
        "    for sentence in root.iter('sentence'):\n",
        "        if \"OutOfScope\" in sentence.attrib and sentence.attrib[\"OutOfScope\"].upper() == \"TRUE\":\n",
        "          continue\n",
        "        text = sentence.find('text').text.strip()\n",
        "        opinions = sentence.find('Opinions')\n",
        "        aspects = []\n",
        "        sentiments = []\n",
        "        if opinions is not None:\n",
        "            for opinion in opinions.findall('Opinion'):\n",
        "                target = opinion.get('target')\n",
        "                polarity = opinion.get('polarity')\n",
        "                if target and target.lower() != \"null\":\n",
        "                  aspects.append(target.lower())\n",
        "                  sentiments.append(polarity)\n",
        "        data.append({\n",
        "            'id': sentence.get('id'),\n",
        "            'text': text,\n",
        "            'aspects': aspects,\n",
        "            'sentiments': sentiments\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=\"\"\n",
        ")\n",
        "\n",
        "def predict_sentiment(sentence_id, sentence):\n",
        "    prompt = (\n",
        "        \"Your task is to do aspect based sentiment analysis.\\n\"\n",
        "        \"Given a sentence, extract all explicit aspect terms from a sentence and determine the sentiment towards each extracted aspect. Use only: positive, negative, neutral.\\n\"\n",
        "        \"Return the result as a JSON object with a single key called 'results', where the value is a list of [aspect, sentiment] pairs.\\n\"\n",
        "        \"Example: \\n\"\n",
        "        \"{\\\"results\\\": [[\\\"ēdiens\\\", \\\"positive\\\"], [\\\"apkalpošana\\\", \\\"neutral\\\"]]}\\n\"\n",
        "        \"If there are no aspect terms in the sentence, return an empty list for 'results'.\\n\"\n",
        "        \"{\\\"results\\\": []}\\n\\n\"\n",
        "        \"Extract the aspect terms exactly as they appear in the sentence, preserving their original form and grammatical case.\\n\"\n",
        "        \"Do not provide any additional text or explanation and do not include implicit aspects or infer anything not explicitly stated.\\n\"\n",
        "        \"Do not include general concepts (e.g., 'cena', 'skaitlis', 'daudzums', 'pieredze').\\n\"\n",
        "        \"If the same aspect appears with mixed sentiment, repeat it for each sentiment (e.g., `baraviku zupa:positive, baraviku zupa:negative`)\\n\"\n",
        "        \"Do not use monetary values or pronouns (e.g., '11€', 'šīs') as aspects.\\n\"\n",
        "        \"Do not include any adjectives, verbs or general sentiment expressions.\\n\"\n",
        "        \"Respond ONLY with valid JSON. Do not add any text before or after.\\n\"\n",
        "        f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
        "    )\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"gemma2-9b-it\"\n",
        "        )\n",
        "        content = completion.choices[0].message.content.strip()\n",
        "        match = re.search(r\"\\{.*\\}\", content, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON in response.\")\n",
        "        parsed = json.loads(match.group(0))\n",
        "        if isinstance(parsed, list):\n",
        "          parsed = parsed[0]\n",
        "        return parsed.get(\"results\", [])\n",
        "\n",
        "# OpenAI. GPT-4o. Uzvedne: varbūt būtu labāk šīs nepareizās atbildes ar exception kļūdām nesaglabāt vispār JSON failā bet kaut kur atsevišķi, jo tad vienkārši JSON failā nebūs kļūdainu rezultātu, kurus nevarēs nolasīt un ar šo rindiņu if sentence_id not in prediction_dict: uzreiz tiks uzskatīta kā kļūda https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] ID {sentence_id} — {e}\")\n",
        "        with open(\"llm_errors_full.log\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"{sentence_id} — {e}\\n\")\n",
        "            f.write(f\"Original response: {completion}\\n\\n\")\n",
        "        return None\n",
        "\n",
        "# OpenAI. GPT-4o. Uzvedne: kā izlabot manā kodā metriku aprēķināšanas funkciju vienlaicīgā aspektu izgūšanā un aspektu noskaņojuma noteikšanā, lai tiktu iegūti TP, FP un FN? aspekti var atkārtoties un tiem varbūt vienādi vai dažādi noskaņojumi https://chatgpt.com/ [izmantots 2025-04-26]\n",
        "from collections import Counter\n",
        "\n",
        "def evaluate_live(test_data):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    skipped = 0\n",
        "    fp_examples = []\n",
        "    fn_examples = []\n",
        "\n",
        "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Processing\"):\n",
        "        true_pairs = [\n",
        "            (lemmatize_phrase(a.lower()), s.lower())\n",
        "            for a, s in zip(row[\"aspects\"], row[\"sentiments\"])\n",
        "        ]\n",
        "\n",
        "        pred_aspects = predict_sentiment(row[\"id\"], row[\"text\"])\n",
        "\n",
        "        if pred_aspects is None:\n",
        "            skipped += 1\n",
        "            FN += len(true_pairs)\n",
        "            continue\n",
        "\n",
        "        pred_pairs = [\n",
        "            (lemmatize_phrase(a[0].lower()), a[1].lower())\n",
        "            for a in pred_aspects if isinstance(a, list) and len(a) == 2\n",
        "        ]\n",
        "\n",
        "        true_counter = Counter(true_pairs)\n",
        "        pred_counter = Counter(pred_pairs)\n",
        "\n",
        "        for pair in (true_counter.keys() | pred_counter.keys()):\n",
        "            true_count = true_counter.get(pair, 0)\n",
        "            pred_count = pred_counter.get(pair, 0)\n",
        "\n",
        "            TP += min(true_count, pred_count)\n",
        "            if pred_count > true_count:\n",
        "                FP += pred_count - true_count\n",
        "                fp_examples.append({\n",
        "                    \"id\": row[\"id\"],\n",
        "                    \"text\": row[\"text\"],\n",
        "                    \"wrong_prediction\": pair\n",
        "                })\n",
        "            if true_count > pred_count:\n",
        "                FN += true_count - pred_count\n",
        "                fn_examples.append({\n",
        "                    \"id\": row[\"id\"],\n",
        "                    \"text\": row[\"text\"],\n",
        "                    \"missing\": pair\n",
        "                })\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"\\nSalīdzināti {len(test_data)} teikumi (izlaisti {skipped})\")\n",
        "    print(f\"True Positives (TP): {TP}\")\n",
        "    print(f\"False Positives (FP): {FP}\")\n",
        "    print(f\"False Negatives (FN): {FN}\")\n",
        "    print(f\"\\nPrecision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 score:  {f1:.4f}\")\n",
        "\n",
        "    with open(\"fp_examples.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fp_examples, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    with open(\"fn_examples.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fn_examples, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\nFP un FN piemēri saglabāti 'fp_examples.json' un 'fn_examples.json'!\")\n",
        "\n",
        "test_data = parse_xml_full_absa(\"LVtestGOLD_1.xml\")\n",
        "evaluate_live(test_data)"
      ],
      "metadata": {
        "id": "rQn1ZL5qlQVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KOMBINĒTAIS UZDEVUMS AR PIEMĒRU UN AR LEMMATIZĀCIJU"
      ],
      "metadata": {
        "id": "DyW75EQXlZF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenRouter. Sample code and API for DeepSeek V3 0324 (free). Tiešsaiste. OpenRouter. Pieejams: https://openrouter.ai/deepseek/deepseek-chat-v3-0324:free/api. [skatīts 2025-03-29].\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: kā man uzlabot šo kodu, lai tas izmantotu no XML izgūtos teikumus, aspektus un noskaņojumus noskaņojuma noteikšanai aspektiem un vēl novērtētu modeļa veiktspēju? https://chatgpt.com/ [izmantots 2025-03-29]\n",
        "# OpenAI. GPT-4o. Uzvedne: kā sasaistīt zemāk dotos kodus un izlabot manu kodu tā, lai pēc katras uzvednes tiktu izgūts rezultāts kurš uzreiz tiek salīdzināts ar attiecīgo pareizo atbildi, nesalīdzinot ar ID, un tad pareizi tiek aprēķinātas metrikas? https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "# OpenAI. GPT-4o. Uzvedne: lemmatizāciju tā, lai lemmatizācija aspektiem tiktu veikta pašā pārbaudē, kur tiek skaitīti TP FP FN? https://chatgpt.com/ [izmantots 2025-05-08]\n",
        "\n",
        "import stanza\n",
        "stanza.download(\"lv\")\n",
        "nlp = stanza.Pipeline(\"lv\", processors=\"tokenize,pos,lemma\", use_gpu=False)\n",
        "def lemmatize_phrase(phrase):\n",
        "    doc = nlp(phrase)\n",
        "    return \" \".join([word.lemma.lower() for sent in doc.sentences for word in sent.words])\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "\n",
        "def parse_xml_full_absa(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    data = []\n",
        "    for sentence in root.iter('sentence'):\n",
        "        if \"OutOfScope\" in sentence.attrib and sentence.attrib[\"OutOfScope\"].upper() == \"TRUE\":\n",
        "          continue\n",
        "        text = sentence.find('text').text.strip()\n",
        "        opinions = sentence.find('Opinions')\n",
        "        aspects = []\n",
        "        sentiments = []\n",
        "        if opinions is not None:\n",
        "            for opinion in opinions.findall('Opinion'):\n",
        "                target = opinion.get('target')\n",
        "                polarity = opinion.get('polarity')\n",
        "                if target and target.lower() != \"null\":\n",
        "                  aspects.append(target.lower())\n",
        "                  sentiments.append(polarity)\n",
        "        data.append({\n",
        "            'id': sentence.get('id'),\n",
        "            'text': text,\n",
        "            'aspects': aspects,\n",
        "            'sentiments': sentiments\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=\"\"\n",
        ")\n",
        "\n",
        "def predict_sentiment(sentence_id, sentence):\n",
        "    prompt = (\n",
        "        \"Your task is to do aspect based sentiment analysis.\\n\"\n",
        "        \"Given a sentence, extract all explicit aspect terms from a sentence and determine the sentiment towards each extracted aspect. Use only: positive, negative, neutral.\\n\"\n",
        "        \"Return the result as a JSON object with a single key called 'results', where the value is a list of [aspect, sentiment] pairs.\\n\"\n",
        "        \"Example: \\n\"\n",
        "        \"{\\\"results\\\": [[\\\"ēdiens\\\", \\\"positive\\\"], [\\\"apkalpošana\\\", \\\"neutral\\\"]]}\\n\"\n",
        "        \"If there are no aspect terms in the sentence, return an empty list for 'results'.\\n\"\n",
        "        \"{\\\"results\\\": []}\\n\\n\"\n",
        "        \"Extract the aspect terms exactly as they appear in the sentence, preserving their original form and grammatical case.\\n\"\n",
        "        \"Do not provide any additional text or explanation and do not include implicit aspects or infer anything not explicitly stated.\\n\"\n",
        "        \"Do not include general concepts (e.g., 'cena', 'skaitlis', 'daudzums', 'pieredze').\\n\"\n",
        "        \"If the same aspect appears with mixed sentiment, repeat it for each sentiment (e.g., `baraviku zupa:positive, baraviku zupa:negative`)\\n\"\n",
        "        \"Do not use monetary values or pronouns (e.g., '11€', 'šīs') as aspects.\\n\"\n",
        "        \"Do not include any adjectives, verbs or general sentiment expressions.\\n\"\n",
        "        \"Respond ONLY with valid JSON. Do not add any text before or after.\\n\"\n",
        "        \"Example:\\n\"\n",
        "        \"Sentence: \\\"Ēdiens bija vidējs līdz virs vidējā; franču sīpolu zupa bija sātīga, bet nekas pārsteidzošs, un deserti nebija nevienā ziņā izcili.\\\"\\n\"\n",
        "        \"Answer: {\\\"results\\\": [[\\\"Ēdiens\\\", \\\"positive\\\"], [\\\"franču sīpolu zupa\\\", \\\"negative\\\"], [\\\"franču sīpolu zupa\\\", \\\"positive\\\"], [\\\"deserti\\\", \\\"negative\\\"]]}\\n\\n\"\n",
        "        f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
        "    )\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"gemma2-9b-it\"\n",
        "        )\n",
        "        content = completion.choices[0].message.content.strip()\n",
        "        match = re.search(r\"\\{.*\\}\", content, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(\"No JSON in response.\")\n",
        "        parsed = json.loads(match.group(0))\n",
        "        if isinstance(parsed, list):\n",
        "          parsed = parsed[0]\n",
        "        return parsed.get(\"results\", [])\n",
        "\n",
        "# OpenAI. GPT-4o. Uzvedne: varbūt būtu labāk šīs nepareizās atbildes ar exception kļūdām nesaglabāt vispār JSON failā bet kaut kur atsevišķi, jo tad vienkārši JSON failā nebūs kļūdainu rezultātu, kurus nevarēs nolasīt un ar šo rindiņu if sentence_id not in prediction_dict: uzreiz tiks uzskatīta kā kļūda https://chatgpt.com/ [izmantots 2025-04-24]\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] ID {sentence_id} — {e}\")\n",
        "        with open(\"llm_errors_full.log\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"{sentence_id} — {e}\\n\")\n",
        "            f.write(f\"Original response: {completion}\\n\\n\")\n",
        "        return None\n",
        "\n",
        "# OpenAI. GPT-4o. Uzvedne: kā izlabot manā kodā metriku aprēķināšanas funkciju vienlaicīgā aspektu izgūšanā un aspektu noskaņojuma noteikšanā, lai tiktu iegūti TP, FP un FN? aspekti var atkārtoties un tiem varbūt vienādi vai dažādi noskaņojumi https://chatgpt.com/ [izmantots 2025-04-26]\n",
        "from collections import Counter\n",
        "\n",
        "def evaluate_live(test_data):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    skipped = 0\n",
        "    fp_examples = []\n",
        "    fn_examples = []\n",
        "\n",
        "    for _, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Processing\"):\n",
        "        true_pairs = [\n",
        "            (lemmatize_phrase(a.lower()), s.lower())\n",
        "            for a, s in zip(row[\"aspects\"], row[\"sentiments\"])\n",
        "        ]\n",
        "\n",
        "        pred_aspects = predict_sentiment(row[\"id\"], row[\"text\"])\n",
        "\n",
        "        if pred_aspects is None:\n",
        "            skipped += 1\n",
        "            FN += len(true_pairs)\n",
        "            continue\n",
        "\n",
        "        pred_pairs = [\n",
        "            (lemmatize_phrase(a[0].lower()), a[1].lower())\n",
        "            for a in pred_aspects if isinstance(a, list) and len(a) == 2\n",
        "        ]\n",
        "\n",
        "        true_counter = Counter(true_pairs)\n",
        "        pred_counter = Counter(pred_pairs)\n",
        "\n",
        "        for pair in (true_counter.keys() | pred_counter.keys()):\n",
        "            true_count = true_counter.get(pair, 0)\n",
        "            pred_count = pred_counter.get(pair, 0)\n",
        "\n",
        "            TP += min(true_count, pred_count)\n",
        "            if pred_count > true_count:\n",
        "                FP += pred_count - true_count\n",
        "                fp_examples.append({\n",
        "                    \"id\": row[\"id\"],\n",
        "                    \"text\": row[\"text\"],\n",
        "                    \"wrong_prediction\": pair\n",
        "                })\n",
        "            if true_count > pred_count:\n",
        "                FN += true_count - pred_count\n",
        "                fn_examples.append({\n",
        "                    \"id\": row[\"id\"],\n",
        "                    \"text\": row[\"text\"],\n",
        "                    \"missing\": pair\n",
        "                })\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"\\nSalīdzināti {len(test_data)} teikumi (izlaisti {skipped})\")\n",
        "    print(f\"True Positives (TP): {TP}\")\n",
        "    print(f\"False Positives (FP): {FP}\")\n",
        "    print(f\"False Negatives (FN): {FN}\")\n",
        "    print(f\"\\nPrecision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 score:  {f1:.4f}\")\n",
        "\n",
        "    with open(\"fp_examples.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fp_examples, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    with open(\"fn_examples.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fn_examples, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\nFP un FN piemēri saglabāti 'fp_examples.json' un 'fn_examples.json'!\")\n",
        "\n",
        "test_data = parse_xml_full_absa(\"LVtestGOLD_1.xml\")\n",
        "evaluate_live(test_data)"
      ],
      "metadata": {
        "id": "pi2SP3bXldaL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}