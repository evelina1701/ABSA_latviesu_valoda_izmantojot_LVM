{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### DATU SAGATAVOŠANA"
      ],
      "metadata": {
        "id": "0yRsEfRvFYaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BrambleXu. aspect term extraction. Tiešsaiste. GitHub: 11.09.2018. Pieejams: https://github.com/BrambleXu/aspect-term-extraction/tree/master. [skatīts 2025-03-23].\n",
        "\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: es tev iedošu kodu, kas tiek izmantots aspektu izgūšanai, un tas ir jāpārveido tā, lai kods derētu noskaņojuma prognozēšanai\n",
        "# noteiktiem aspektiem teikumā. modelim nepieciešams prast tikai noteikt noskaņojumu jau dotajam aspektam, nevis noteikt gan aspektu, gan noskaņojumu. arī sākumā datu\n",
        "# priekšapstrādei jānotiek tā, lai priekšapstrādātie dati tiktu saglabāti CSV failā, kas vēlāk tiek izmantots modeļa apmācīšanai kā tas ir redzams zemāk dotajā\n",
        "# kodā https://chatgpt.com/ [izmantots 2025-03-27]\n",
        "def soup2dict_sentiment(sentence_nodes):\n",
        "    \"\"\"\n",
        "    Input: soup objects\n",
        "    Output: a list of dictionaries, contains 'id', 'text', 'aspect', 'sentiment'\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    i = 0\n",
        "    for n in sentence_nodes:\n",
        "        i += 1\n",
        "        sentence = {}\n",
        "        aspect_term = []\n",
        "        polarity_list = []\n",
        "        sentence['id'] = i\n",
        "        sentence['text'] = n.find('text').string\n",
        "        if n.find('Opinions'):\n",
        "            for c in n.find('Opinions').contents:\n",
        "                if c.name == 'Opinion':\n",
        "                    aspect_term.append(c['target'])\n",
        "                    polarity_list.append(c['polarity'])\n",
        "        sentence['aspect'] = aspect_term\n",
        "        sentence['sentiment'] = polarity_list\n",
        "        sentences.append(sentence)\n",
        "\n",
        "    return sentences\n"
      ],
      "metadata": {
        "id": "WbRrwFdIFUmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split2words(s_text):\n",
        "    \"\"\"Split string with white and prereserve the punctuation\n",
        "    Input:\n",
        "        s_text: string, a sentence, e.g. Judging from previous posts this used to be a good place, but not any longer.\n",
        "    Output:\n",
        "        words: a list of words, e.g. ['judging', 'from', 'previous', 'posts', 'this', 'used', 'to', 'be', 'a', 'good', 'place', ',', 'but', 'not', 'any', 'longer', '.']\n",
        "    \"\"\"\n",
        "    s_text = re.sub('([.,!?()])', r' \\1 ', s_text) # match the punctuation characters and surround them by spaces,\n",
        "    s_text = re.sub('\\s{2,}', ' ', s_text)         # collapse multiple spaces to one space\n",
        "    words = s_text.lower().split()\n",
        "    return words\n"
      ],
      "metadata": {
        "id": "pJmzjXnqFdrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: pārveido šo kodu tā, lai tas dotu katram vārdam noskaņojuma marķierus atkarībā no dotā noskaņojuma:\n",
        "# positive: 2, neutral: 1, negative: 0 https://chatgpt.com/ [izmantots 2025-03-27]\n",
        "def tagging_sentiment(words, aspects, sentiment):\n",
        "    \"\"\"\n",
        "    Assigning sentiment-based tag to each word in `words`,\n",
        "    atkarībā no doto aspektu saraksta un sentimenta.\n",
        "\n",
        "    Input:\n",
        "        words: a list of words, e.g. ['zaļās', 'tējas', 'krēms', 'brulē', ...]\n",
        "        aspects: a list of aspect term, e.g. ['zaļās tējas krēms brulē']\n",
        "        sentiment: one of ['positive', 'neutral', 'negative']\n",
        "\n",
        "    Output:\n",
        "        tags: a list which length = len(words).\n",
        "              Each element value can be 'POS' (positive), 'NEU' (neutral), 'NEG' (negative) or 'O' if a word is not an aspect.\n",
        "    \"\"\"\n",
        "    tags = ['O'] * len(words)\n",
        "\n",
        "    sent_map = {'positive': 'POS', 'neutral': 'NEU', 'negative': 'NEG'}\n",
        "    label = sent_map.get(sentiment, 'O')\n",
        "\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: ir tāda nianse, ka, protams, aspekta var nebūt teikumā vispār, tad tas ir atzīmēts kā NULL, bet jebkurā gadījumā\n",
        "# tam ir noskaņojums, kurš ir jānosaka. tāpat arī, ja pirmais aspekts ir NULL, tas nenozīmē, ka teikumā vispār nav aspektu. teikums var būt garš, un pirmajā\n",
        "# teikuma daļā var nebūt aspekta, bet otrajā teikuma daļā var būt aspekts, tāpēc šis pieņēmums, ka,  ja pirmais aspekts ir NULL, tad pārējā teikumā nav aspektu.\n",
        "# tev attiecīgi jāizlabo tagging_sentiment un dict2df, lai atbilstu tam, ko es tev minēju augstāk https://chatgpt.com/ [izmantots 2025-03-27]\n",
        "    valid_aspects = [a for a in aspects if a != 'NULL']\n",
        "\n",
        "    if not valid_aspects:\n",
        "        return tags\n",
        "\n",
        "    for aspect in valid_aspects:\n",
        "        aspect_lower = aspect.lower()\n",
        "        for i, w in enumerate(words):\n",
        "            if w in aspect_lower:\n",
        "                tags[i] = label\n",
        "    return tags\n"
      ],
      "metadata": {
        "id": "_r1GHF8FFe5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: tagad atbilstoši zemāk dotajiem kodiem, jāizmaina dict2df funkciju https://chatgpt.com/ [izmantots 2025-03-27]\n",
        "def dict2df(sentences):\n",
        "    \"\"\"\n",
        "    Convert list of dict to dataframe\n",
        "    Input:\n",
        "        sentences: a list of dictionaries, e.g.:\n",
        "          {\n",
        "            'id': <sentence ID>,\n",
        "            'text': <full sentence>,\n",
        "            'aspect': <a list of aspects>,\n",
        "            'sentiment': <a list of sentiments, e.g. ['positive']>\n",
        "          }\n",
        "    Output:\n",
        "        DataFrame with columns [\"Sentence #\", \"Word\", \"Tag\"]\n",
        "        - \"Word\" is a tokenized sentence (lowercase, spaces between punctuation marks),\n",
        "        - \"Tag\" contains marker ('POS', 'NEU', 'NEG') or 'O'.\n",
        "    \"\"\"\n",
        "    data_frames = []\n",
        "\n",
        "    for s in sentences:\n",
        "        sentence = {}\n",
        "        sentence['Sentence #'] = s['id']\n",
        "\n",
        "        words = split2words(s['text'])\n",
        "        sentence['Word'] = words\n",
        "\n",
        "        if 'sentiment' in s and len(s['sentiment']) > 0:\n",
        "            sentiment_label = s['sentiment'][0]\n",
        "        else:\n",
        "            sentiment_label = 'neutral'\n",
        "\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: ir tāda nianse, ka, protams, aspekta var nebūt teikumā vispār, tad tas ir atzīmēts kā NULL, bet jebkurā gadījumā\n",
        "# tam ir noskaņojums, kurš ir jānosaka. tāpat arī, ja pirmais aspekts ir NULL, tas nenozīmē, ka teikumā vispār nav aspektu. teikums var būt garš, un pirmajā\n",
        "# teikuma daļā var nebūt aspekta, bet otrajā teikuma daļā var būt aspekts, tāpēc šis pieņēmums, ka,  ja pirmais aspekts ir NULL, tad pārējā teikumā nav aspektu.\n",
        "# tev attiecīgi jāizlabo tagging_sentiment un dict2df, lai atbilstu tam, ko es tev minēju augstāk https://chatgpt.com/ [izmantots 2025-03-27]\n",
        "        aspect_terms = s['aspect'] if 'aspect' in s else []\n",
        "        aspect_terms = [a.lower() for a in aspect_terms]\n",
        "        tags = tagging_sentiment(words, aspect_terms, sentiment_label)\n",
        "\n",
        "        sentence['Tag'] = tags\n",
        "\n",
        "        sentence_df = pd.DataFrame(sentence)\n",
        "        data_frames.append(sentence_df)\n",
        "\n",
        "    data = pd.concat(data_frames, ignore_index=True)\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "R39o-MT7Fhob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file_path):\n",
        "    # 1 raw data to soup\n",
        "    soup = None\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        soup = BeautifulSoup(f.read().strip(), \"lxml-xml\")\n",
        "    if soup is None:\n",
        "        raise Exception(\"Can't read xml file\")\n",
        "    sentence_nodes = soup.find_all(\"sentence\")\n",
        "\n",
        "    # 2  convert soup object to a list of dictionaries\n",
        "    sentences = soup2dict_sentiment(sentence_nodes)\n",
        "\n",
        "    # 3 list to dataframe\n",
        "    data = dict2df(sentences)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "YaStA0MVFl47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from bs4 import BeautifulSoup\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: izmaini šo koda gabalu tā, lai tas darbotos google colab vidē https://chatgpt.com/ [izmantots 2025-03-23]\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "data = read_data(file_name)\n",
        "data.tail()"
      ],
      "metadata": {
        "id": "Yb1z1mqBFnJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_file = Path('train.csv')\n",
        "data.to_csv(save_file, index=False)"
      ],
      "metadata": {
        "id": "vsUKDqp1FoUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM un BILSTM apmācīšana un pārbaudīšana"
      ],
      "metadata": {
        "id": "CcLQCpIpFU6_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4B5zWevhOQD"
      },
      "outputs": [],
      "source": [
        "# BrambleXu. aspect term extraction. Tiešsaiste. GitHub: 11.09.2018. Pieejams: https://github.com/BrambleXu/aspect-term-extraction/tree/master. [skatīts 2025-03-23].\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from pathlib import Path\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data_train = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_train\n",
        "# Save all words as a list\n",
        "words = list(set(data['Word'].values))\n",
        "n_words = len(words)\n",
        "\n",
        "tags = list(set(data[\"Tag\"].values))\n",
        "n_tags = len(tags)\n",
        "\n",
        "max_len = 75\n",
        "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
        "n_words = len(word2idx)\n",
        "# word2idx['<unk>'] = len(word2idx) + 1\n",
        "tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2idx['<pad>'] = 0\n",
        "n_tags = len(tag2idx) # Due to <pad>, here total tag number is from 17 to 18"
      ],
      "metadata": {
        "id": "lt2qZR3Yhf6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence class\n",
        "class SentenceGetter(object):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
        "#                                                            s[\"POS\"].values.tolist(),\n",
        "                                                           s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "\n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "getter = SentenceGetter(data)\n",
        "sentences = getter.sentences # get all sentences\n",
        "\n",
        "# Word2inx & Padding for X\n",
        "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
        "X_train = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0)\n",
        "\n",
        "# Word2inx & Padding for y\n",
        "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=0)\n",
        "\n",
        "# Get one-hot labels\n",
        "y_train = [to_categorical(i, num_classes=n_tags) for i in y]\n",
        "\n",
        "print(tag2idx)\n",
        "print(len(X_train))\n",
        "print(len(y_train))"
      ],
      "metadata": {
        "id": "B20vD5gJhkdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: ievēro iepriekš sūtīto ziņu ar ieteikumiem, lai ieviestu kodā izmaiņas, kur netiek ņemti vērā True Negatives jeb 'O' un\n",
        "# kur Precision un Recall tiek rēķināti tikai B un I klasēm, un to pašu principu ievies arī zudumu aprēķinu apmācības procesā veic izmaiņas tieši manis dotajā\n",
        "# kodā https://chatgpt.com/ [izmantots 2025-04-01]\n",
        "def custom_loss(y_true, y_pred):\n",
        "    o_index = tag2idx.get(\"O\")\n",
        "    if o_index is None:\n",
        "        raise ValueError(\"O label not found in tag2idx.\")\n",
        "    mask = tf.cast(tf.not_equal(tf.argmax(y_true, axis=-1), o_index), dtype=tf.float32)\n",
        "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "    loss = loss * mask\n",
        "    return tf.reduce_sum(loss) / (tf.reduce_sum(mask) + 1e-7)\n"
      ],
      "metadata": {
        "id": "gAXNUjQuF7UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: kā var ieviest manā kodā klašu svarus? https://chatgpt.com/ [izmantots 2025-03-26]\n",
        "y_train = np.array(y_train)\n",
        "y_train_int = np.argmax(y_train, axis=-1)\n",
        "\n",
        "class_weight_dict_idx = {\n",
        "    tag2idx['O']: 1.0,\n",
        "    tag2idx['NEG']: 15.0,\n",
        "    tag2idx['NEU']: 17.0,\n",
        "    tag2idx['POS']: 7.0\n",
        "}\n",
        "print(tag2idx)\n",
        "\n",
        "import numpy as np\n",
        "sample_weight = np.ones((y_train_int.shape[0], y_train_int.shape[1]), dtype='float32')\n",
        "for i in range(y_train_int.shape[0]):\n",
        "    for j in range(y_train_int.shape[1]):\n",
        "        cls_idx = y_train_int[i,j]\n",
        "        if cls_idx in class_weight_dict_idx:\n",
        "            sample_weight[i,j] = class_weight_dict_idx[cls_idx]\n",
        "        else:\n",
        "            sample_weight[i,j] = 0.0"
      ],
      "metadata": {
        "id": "pDyvcsP9hsl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GeeksforGeeks. Long short-term memory (LSTM) RNN in Tensorflow. Tiešsaiste. GeeksforGeeks: 25.02.2025. Pieejams: https://www.geeksforgeeks.org/long-short-term-memory-lstm-rnn-in-tensorflow/. [skatīts 2025-03-23].\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: par pamatu ņēmu kodu no šī avota (https://github.com/BrambleXu/aspect-term-extraction/blob/master/notebooks/bi-lstm-crf.ipynb).\n",
        "# ko vari ieteikt, lai varētu uzlabot manu kodu? jo modelis arī apmācās ļoti lēni, salīdzinājumā ar github repositorija autora rezultātiem, un metriku rezultāti manā\n",
        "# gadījumā arī ir zemi https://chatgpt.com/ [izmantots 2025-03-24]\n",
        "input_layer = Input(shape=(max_len,))\n",
        "model = Embedding(input_dim=len(word2idx) + 1,\n",
        "                  output_dim=200,\n",
        "                  input_length=max_len,\n",
        "                  mask_zero=True,\n",
        "                  trainable=True)(input_layer)\n",
        "model = LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)(model)\n",
        "model = Dropout(0.2)(model)\n",
        "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)\n",
        "model = Model(input_layer, out)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=custom_loss,\n",
        "              metrics=[\"accuracy\"], run_eagerly=True)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "y9IKCMS_h-a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: tagad nepieciešams sagatavot kodu ar BiLSTM algoritmu, es tev iedošu savu tagadējo kodu ar tagadējiem rezultātiem,\n",
        "# nepieciešamības pēc veic labojumus un sniedz ieteikumus https://chatgpt.com/ [izmantots 2025-03-27]\n",
        "input_layer = Input(shape=(max_len,))\n",
        "model = Embedding(input_dim=len(word2idx) + 1,\n",
        "                  output_dim=300,\n",
        "                  input_length=max_len,\n",
        "                  mask_zero=True)(input_layer)\n",
        "model = Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1))(model)\n",
        "model = Dropout(0.2)(model)\n",
        "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)\n",
        "\n",
        "model = Model(input_layer, out)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=custom_loss,\n",
        "              metrics=[\"accuracy\"], run_eagerly=True)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "r8vH4KifDdYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, np.array(y_train), batch_size=256, epochs=12,\n",
        "                    validation_split=0.1, sample_weight=sample_weight, verbose=1)\n",
        "hist = pd.DataFrame(history.history)\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1QlKJYiXiJWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODEĻU PĀRBAUDĪŠANA"
      ],
      "metadata": {
        "id": "_hZ-74zHGKfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_test\n",
        "getter = SentenceGetter(data)\n",
        "sentences = getter.sentences # get all sentences\n",
        "\n",
        "# Word2inx & Padding for X\n",
        "X = [[word2idx.get(w[0], 0) for w in s] for s in sentences]\n",
        "X_test = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0)\n",
        "\n",
        "# Word2inx & Padding for y\n",
        "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx['<pad>'])\n",
        "\n",
        "# Get one-hot labels\n",
        "y_test = [to_categorical(i, num_classes=n_tags) for i in y]"
      ],
      "metadata": {
        "id": "b9sbvtdziKb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[1])\n",
        "print(X_test[1])\n",
        "print(np.argmax(y_test[1], -1))"
      ],
      "metadata": {
        "id": "NWK_iIoXiNy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions.\n",
        "idx2word = {value: key for key, value in word2idx.items()}\n",
        "idx2tag = {value: key for key, value in tag2idx.items()}\n",
        "print(idx2tag)"
      ],
      "metadata": {
        "id": "Xrwvaxe4iPJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_all = np.argmax(y_test, -1)\n",
        "true_all_tags = [[idx2tag[idx] for idx in s if idx!=0] for s in true_all]\n",
        "\n",
        "p_all = model.predict(np.array(X_test)) # (4796, 75, 18)\n",
        "p_all= np.argmax(p_all, axis=-1) # (4796, 75)\n",
        "p_all_tags = [[idx2tag[idx] for idx in s] for s in p_all] # ['B-gpe', 'O', 'O', 'O']\n",
        "\n",
        "for i, true in enumerate(true_all_tags):\n",
        "    length = len(true)\n",
        "    p_all_tags[i] = p_all_tags[i][:length]\n",
        "\n",
        "p_all_tags = [[x.replace('<pad>', 'O') for x in s] for s in p_all_tags]\n",
        "\n",
        "print(sentences[3])\n",
        "print(X_test[3])\n",
        "print(true_all_tags[3])\n",
        "print(p_all_tags[3])"
      ],
      "metadata": {
        "id": "gcGFITtxiRhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: kā man izlabot kļūdu ValueError: max() arg is an empty sequence šajā koda daļā? https://chatgpt.com/ [izmantots 2025-03-26]\n",
        "# OpenAI. ChatGPT o3-mini-high. Uzvedne: šo kodu es izmantoju modeļa apmācīšanai ar datu kopu, kas ir paredzēta noskaņojumu noteikšanai aspektiem.\n",
        "# CSV faili sastāv no teikuma numura, vārda un marķiera, kas apzīmē šī vārda noskaņojumu (2 - pozitīvs, 1 - neitrāls, 0 - negatīvs, O - nav noskaņojuma) https://chatgpt.com/ [izmantots 2025-03-26]\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "true_flat = []\n",
        "pred_flat = []\n",
        "for i in range(len(true_all_tags)):\n",
        "    true_flat.extend(true_all_tags[i])\n",
        "    pred_flat.extend(p_all_tags[i])\n",
        "\n",
        "print(classification_report(true_flat, pred_flat, labels=[\"NEG\",\"NEU\",\"POS\",\"O\"]))\n"
      ],
      "metadata": {
        "id": "8mUFylSziTyk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}