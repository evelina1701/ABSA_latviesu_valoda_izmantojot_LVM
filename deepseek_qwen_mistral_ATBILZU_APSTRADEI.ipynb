{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### DEEPSEEK UN QWEN MODEĻU ATBILŽU PAPILDUS APSTRĀDEI ASPEKTU IZGŪŠANĀ"
      ],
      "metadata": {
        "id": "iYtfMMLFlpo7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXGR7ZiMa2rn"
      },
      "outputs": [],
      "source": [
        "# OpenAI. GPT-4o. Uzvedne: pēc visām izskaidrotajām niansēm uzraksti pilnvērtīgu kodu, kas ļautu šīs atbildes apstrādāt. TP FP FN es ievietošu pašā kodā piem TP=476, FP=256, FN=123 https://chatgpt.com/ [izmantots 2025-05-08]\n",
        "import re\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "TP = 133\n",
        "FP = 158\n",
        "FN = 377\n",
        "\n",
        "log_file = \"llm_errors_aspects.log\"\n",
        "xml_file = \"LVtestGOLD_1.xml\"\n",
        "\n",
        "def get_gold_aspects(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    gold_data = {}\n",
        "    for sentence in root.iter(\"sentence\"):\n",
        "        sent_id = sentence.get(\"id\")\n",
        "        if sentence.find(\"Opinions\") is not None:\n",
        "            aspects = []\n",
        "            for op in sentence.find(\"Opinions\").findall(\"Opinion\"):\n",
        "                target = op.get(\"target\")\n",
        "                if target and target.lower() != \"null\":\n",
        "                    aspects.append(target.lower())\n",
        "            if aspects:\n",
        "                gold_data[sent_id] = aspects\n",
        "    return gold_data\n",
        "\n",
        "def extract_last_json(text):\n",
        "    matches = re.findall(r\"\\{[\\s\\S]*?\\\"results\\\"\\s*:\\s*\\[.*?\\]\\}\", text)\n",
        "    if matches:\n",
        "        try:\n",
        "            return json.loads(matches[-1])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "gold_aspects_dict = get_gold_aspects(xml_file)\n",
        "errors = []\n",
        "\n",
        "with open(log_file, encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "entries = re.split(r\"\\n(?=\\w+_[^:]+:\\d+ — Extra data)\", content)\n",
        "\n",
        "for entry in entries:\n",
        "    if not entry.strip():\n",
        "        continue\n",
        "\n",
        "    match_id = re.match(r\"(\\w+_\\w+_\\d+:\\d+)\", entry)\n",
        "    if not match_id:\n",
        "        continue\n",
        "\n",
        "    sent_id = match_id.group(1)\n",
        "    gold_aspects = gold_aspects_dict.get(sent_id, [])\n",
        "\n",
        "    FN -= len(gold_aspects)\n",
        "\n",
        "    predicted = extract_last_json(entry)\n",
        "    if predicted is None or \"results\" not in predicted:\n",
        "        FN += len(gold_aspects)\n",
        "        errors.append({\n",
        "            \"id\": sent_id,\n",
        "            \"error_type\": \"NO_JSON\",\n",
        "            \"predicted\": None,\n",
        "            \"gold\": gold_aspects\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    predicted_aspects = [asp.lower() for asp in predicted[\"results\"]]\n",
        "    gold_set = set(gold_aspects)\n",
        "    pred_set = set(predicted_aspects)\n",
        "\n",
        "    # TP: korekti atrastie aspekti\n",
        "    tp_aspects = gold_set & pred_set\n",
        "    TP += len(tp_aspects)\n",
        "\n",
        "    # FP: modeļa liekie aspekti\n",
        "    fp_aspects = pred_set - gold_set\n",
        "    FP += len(fp_aspects)\n",
        "    for asp in fp_aspects:\n",
        "        errors.append({\n",
        "            \"id\": sent_id,\n",
        "            \"error_type\": \"FP\",\n",
        "            \"predicted\": asp,\n",
        "            \"gold\": gold_aspects\n",
        "        })\n",
        "\n",
        "    # FN: aspekti, kurus modelis izlaida\n",
        "    fn_aspects = gold_set - pred_set\n",
        "    FN += len(fn_aspects)\n",
        "    for asp in fn_aspects:\n",
        "        errors.append({\n",
        "            \"id\": sent_id,\n",
        "            \"error_type\": \"FN\",\n",
        "            \"missed_gold_aspect\": asp,\n",
        "            \"predicted\": predicted_aspects\n",
        "        })\n",
        "\n",
        "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
        "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "print(\"\\nPĀRRĒĶINĀTĀS METRIKAS:\")\n",
        "print(f\"TP: {TP}\")\n",
        "print(f\"FP: {FP}\")\n",
        "print(f\"FN: {FN}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 score:  {f1:.4f}\")\n",
        "\n",
        "with open(\"adjusted_llm_errors.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(errors, f, ensure_ascii=False, indent=2)\n",
        "print(\"\\nSaglabātas kļūdas failā: adjusted_llm_errors.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MISTRAL MODEĻA ATBILŽU PAPILDUS APSTRĀDEI ASPEKTU IZGŪŠANĀ"
      ],
      "metadata": {
        "id": "Xqn8GsnYisSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. GPT-4o. Uzvedne: pēc visām izskaidrotajām niansēm uzraksti pilnvērtīgu kodu, kas ļautu šīs atbildes apstrādāt. TP FP FN es ievietošu pašā kodā piem TP=476, FP=256, FN=123 https://chatgpt.com/ [izmantots 2025-05-08]\n",
        "\n",
        "import re\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "TP = 191\n",
        "FP = 334\n",
        "FN = 311\n",
        "\n",
        "log_file = \"llm_errors_aspects.log\"\n",
        "xml_file = \"LVtestGOLD_1.xml\"\n",
        "\n",
        "def get_gold_aspects(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    gold_data = {}\n",
        "    for sentence in root.iter(\"sentence\"):\n",
        "        sent_id = sentence.get(\"id\")\n",
        "        if sentence.find(\"Opinions\") is not None:\n",
        "            aspects = []\n",
        "            for op in sentence.find(\"Opinions\").findall(\"Opinion\"):\n",
        "                target = op.get(\"target\")\n",
        "                if target and target.lower() != \"null\":\n",
        "                    aspects.append(target.lower())\n",
        "            if aspects:\n",
        "                gold_data[sent_id] = aspects\n",
        "    return gold_data\n",
        "\n",
        "def extract_first_json(text):\n",
        "    matches = re.findall(r\"\\{[\\s\\S]*?\\\"results\\\"\\s*:\\s*\\[.*?\\]\\}\", text)\n",
        "    if matches:\n",
        "        try:\n",
        "            return json.loads(matches[0])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "gold_aspects_dict = get_gold_aspects(xml_file)\n",
        "errors = []\n",
        "\n",
        "with open(log_file, encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "entries = re.split(r\"\\n(?=\\w+_[^:]+:\\d+ — Extra data)\", content)\n",
        "\n",
        "for entry in entries:\n",
        "    if not entry.strip():\n",
        "        continue\n",
        "\n",
        "    match_id = re.match(r\"(\\w+_\\w+_\\d+:\\d+)\", entry)\n",
        "    if not match_id:\n",
        "        continue\n",
        "\n",
        "    sent_id = match_id.group(1)\n",
        "    gold_aspects = gold_aspects_dict.get(sent_id, [])\n",
        "\n",
        "    FN -= len(gold_aspects)\n",
        "\n",
        "    predicted = extract_first_json(entry)\n",
        "    if predicted is None or \"results\" not in predicted:\n",
        "        FN += len(gold_aspects)\n",
        "        errors.append({\n",
        "            \"id\": sent_id,\n",
        "            \"error_type\": \"NO_JSON\",\n",
        "            \"predicted\": None,\n",
        "            \"gold\": gold_aspects\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    predicted_aspects = [asp.lower() for asp in predicted[\"results\"]]\n",
        "    gold_set = set(gold_aspects)\n",
        "    pred_set = set(predicted_aspects)\n",
        "\n",
        "    # TP: korekti atrastie aspekti\n",
        "    tp_aspects = gold_set & pred_set\n",
        "    TP += len(tp_aspects)\n",
        "\n",
        "    # FP: liekie aspekti\n",
        "    fp_aspects = pred_set - gold_set\n",
        "    FP += len(fp_aspects)\n",
        "    for asp in fp_aspects:\n",
        "        errors.append({\n",
        "            \"id\": sent_id,\n",
        "            \"error_type\": \"FP\",\n",
        "            \"predicted\": asp,\n",
        "            \"gold\": gold_aspects\n",
        "        })\n",
        "\n",
        "    # FN: aspekti, ko modelis neizgūst\n",
        "    fn_aspects = gold_set - pred_set\n",
        "    FN += len(fn_aspects)\n",
        "    for asp in fn_aspects:\n",
        "        errors.append({\n",
        "            \"id\": sent_id,\n",
        "            \"error_type\": \"FN\",\n",
        "            \"missed_gold_aspect\": asp,\n",
        "            \"predicted\": predicted_aspects\n",
        "        })\n",
        "\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(\"\\nPĀRRĒĶINĀTĀS METRIKAS (Mistral):\")\n",
        "print(f\"TP: {TP}\")\n",
        "print(f\"FP: {FP}\")\n",
        "print(f\"FN: {FN}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 score:  {f1:.4f}\")\n",
        "\n",
        "with open(\"adjusted_llm_errors_mistral.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(errors, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nSaglabātas kļūdas failā: adjusted_llm_errors_mistral.json\")\n"
      ],
      "metadata": {
        "id": "ipK5Weuuir4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DEEPSEEK UN QWEN MODEĻU ATBILŽU PAPILDUS APSTRĀDEI ASPEKTU NOSKAŅOJUMU NOTEIKŠANĀ"
      ],
      "metadata": {
        "id": "Bb1rmMWbonjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. GPT-4o. Uzvedne: nepieciešams pārveidot šo kodu tā, lai pēc tāda paša principa tiktu pārbaudīti izgūtie noskaņojumi no modeļu garajām atbildēm https://chatgpt.com/ [izmantots 2025-05-08]\n",
        "\n",
        "import re\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "TP = 476\n",
        "FP = 65\n",
        "FN = 173\n",
        "\n",
        "log_file = \"llm_errors_sentiments.log\"\n",
        "xml_file = \"testGOLD2016_1.xml\"\n",
        "\n",
        "def get_gold_sentiments(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    gold_data = {}\n",
        "    for sentence in root.iter(\"sentence\"):\n",
        "        sent_id = sentence.get(\"id\")\n",
        "        aspects = []\n",
        "        sentiments = []\n",
        "        if sentence.find(\"Opinions\") is not None:\n",
        "            for op in sentence.find(\"Opinions\").findall(\"Opinion\"):\n",
        "                target = op.get(\"target\")\n",
        "                polarity = op.get(\"polarity\")\n",
        "                if target and target.lower() != \"null\":\n",
        "                    aspects.append(target.lower())\n",
        "                    sentiments.append(polarity.lower())\n",
        "        if aspects:\n",
        "            gold_data[sent_id] = sentiments\n",
        "    return gold_data\n",
        "\n",
        "def extract_last_json(text):\n",
        "    matches = re.findall(r\"\\{[\\s\\S]*?\\\"results\\\"\\s*:\\s*\\[.*?\\]\\}\", text)\n",
        "    if matches:\n",
        "        try:\n",
        "            return json.loads(matches[-1])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "gold_sentiments_dict = get_gold_sentiments(xml_file)\n",
        "errors = []\n",
        "\n",
        "with open(log_file, encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "entries = re.split(r\"\\n(?=\\w+_[^:]+:\\d+ — Extra data)\", content)\n",
        "\n",
        "for entry in entries:\n",
        "    if not entry.strip():\n",
        "        continue\n",
        "\n",
        "    match_id = re.match(r\"(\\w+_\\w+_\\d+:\\d+)\", entry)\n",
        "    if not match_id:\n",
        "        continue\n",
        "\n",
        "    sent_id = match_id.group(1)\n",
        "    gold_sentiments = gold_sentiments_dict.get(sent_id, [])\n",
        "\n",
        "    FN -= len(gold_sentiments)\n",
        "\n",
        "    predicted = extract_last_json(entry)\n",
        "    if predicted is None or \"results\" not in predicted:\n",
        "        FN += len(gold_sentiments)\n",
        "        errors.append({\n",
        "            \"id\": sent_id,\n",
        "            \"error_type\": \"NO_JSON\",\n",
        "            \"predicted\": None,\n",
        "            \"gold\": gold_sentiments\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    predicted_sentiments = predicted[\"results\"]\n",
        "    valid_labels = {\"positive\", \"negative\", \"neutral\"}\n",
        "    corrected_preds = [s.lower().strip() for s in predicted_sentiments if isinstance(s, str)]\n",
        "\n",
        "    for i, true_label in enumerate(gold_sentiments):\n",
        "        try:\n",
        "            pred_label = corrected_preds[i]\n",
        "        except IndexError:\n",
        "            FN += 1\n",
        "            errors.append({\n",
        "                \"id\": sent_id,\n",
        "                \"error_type\": \"Missing sentiment\",\n",
        "                \"true_sentiment\": true_label,\n",
        "                \"predicted_sentiment\": None\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        if pred_label not in valid_labels:\n",
        "            FN += 1\n",
        "            errors.append({\n",
        "                \"id\": sent_id,\n",
        "                \"error_type\": \"Invalid label\",\n",
        "                \"true_sentiment\": true_label,\n",
        "                \"predicted_sentiment\": pred_label\n",
        "            })\n",
        "        elif pred_label == true_label:\n",
        "            TP += 1\n",
        "        else:\n",
        "            FP += 1\n",
        "            FN += 1\n",
        "            errors.append({\n",
        "                \"id\": sent_id,\n",
        "                \"error_type\": \"Incorrect sentiment\",\n",
        "                \"true_sentiment\": true_label,\n",
        "                \"predicted_sentiment\": pred_label\n",
        "            })\n",
        "\n",
        "    if len(corrected_preds) > len(gold_sentiments):\n",
        "        for extra in corrected_preds[len(gold_sentiments):]:\n",
        "            FP += 1\n",
        "            errors.append({\n",
        "                \"id\": sent_id,\n",
        "                \"error_type\": \"Extra prediction\",\n",
        "                \"true_sentiment\": None,\n",
        "                \"predicted_sentiment\": extra\n",
        "            })\n",
        "\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(\"\\nPĀRRĒĶINĀTĀS SENTIMENTU METRIKAS:\")\n",
        "print(f\"TP: {TP}\")\n",
        "print(f\"FP: {FP}\")\n",
        "print(f\"FN: {FN}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 score:  {f1:.4f}\")\n",
        "\n",
        "with open(\"adjusted_llm_sentiment_errors.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(errors, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nSaglabātas kļūdas failā: adjusted_llm_sentiment_errors.json\")\n"
      ],
      "metadata": {
        "id": "GZrCCQanopZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DEEPSEEK UN QWEN MODEĻU ATBILŽU PAPILDUS APSTRĀDEI ASPEKTU IZGŪŠANĀ AR LEMMATIZĀCIJU"
      ],
      "metadata": {
        "id": "UJ072wTTwv4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install stanza"
      ],
      "metadata": {
        "id": "OqXJB1u4xyIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. GPT-4o. Uzvedne: pēc visām izskaidrotajām niansēm uzraksti pilnvērtīgu kodu, kas ļautu šīs atbildes apstrādāt. TP FP FN es ievietošu pašā kodā piem TP=476, FP=256, FN=123 https://chatgpt.com/ [izmantots 2025-05-08]\n",
        "# OpenAI. GPT-4o. Uzvedne: kā ieviest lemmatizāciju tā, lai lemmatizācija aspektiem tiktu veikta pašā pārbaudē, kur tiek skaitīti TP FP FN? https://chatgpt.com/ [izmantots 2025-05-08]\n",
        "\n",
        "import stanza\n",
        "\n",
        "stanza.download(\"lv\")\n",
        "nlp = stanza.Pipeline(\"lv\", processors=\"tokenize,pos,lemma\", tokenize_no_ssplit=True)\n",
        "def lemmatize_phrase(text):\n",
        "    doc = nlp(text.lower())\n",
        "    return \" \".join([word.lemma for sent in doc.sentences for word in sent.words])\n",
        "\n",
        "\n",
        "import re\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "TP = 147\n",
        "FP = 124\n",
        "FN = 495\n",
        "\n",
        "log_file = \"llm_errors_aspects.log\"\n",
        "xml_file = \"testGOLD2016_1.xml\"\n",
        "\n",
        "def get_gold_aspects(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    gold_data = {}\n",
        "    for sentence in root.iter(\"sentence\"):\n",
        "        sent_id = sentence.get(\"id\")\n",
        "        if sentence.find(\"Opinions\") is not None:\n",
        "            aspects = []\n",
        "            for op in sentence.find(\"Opinions\").findall(\"Opinion\"):\n",
        "                target = op.get(\"target\")\n",
        "                if target and target.lower() != \"null\":\n",
        "                    aspects.append(target.lower())\n",
        "            if aspects:\n",
        "                gold_data[sent_id] = aspects\n",
        "    return gold_data\n",
        "\n",
        "def extract_last_json(text):\n",
        "    matches = re.findall(r\"\\{[\\s\\S]*?\\\"results\\\"\\s*:\\s*\\[.*?\\]\\}\", text)\n",
        "    if matches:\n",
        "        try:\n",
        "            return json.loads(matches[-1])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "gold_aspects_dict = get_gold_aspects(xml_file)\n",
        "errors = []\n",
        "\n",
        "with open(log_file, encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "entries = re.split(r\"\\n(?=\\w+_[^:]+:\\d+ — Extra data)\", content)\n",
        "\n",
        "for entry in entries:\n",
        "    if not entry.strip():\n",
        "        continue\n",
        "\n",
        "    match_id = re.match(r\"(\\w+_\\w+_\\d+:\\d+)\", entry)\n",
        "    if not match_id:\n",
        "        continue\n",
        "\n",
        "    sent_id = match_id.group(1)\n",
        "    gold_aspects = gold_aspects_dict.get(sent_id, [])\n",
        "\n",
        "    FN -= len(gold_aspects)\n",
        "\n",
        "    predicted = extract_last_json(entry)\n",
        "    if predicted is None or \"results\" not in predicted:\n",
        "        FN += len(gold_aspects)\n",
        "        errors.append({\n",
        "            \"id\": sent_id,\n",
        "            \"error_type\": \"NO_JSON\",\n",
        "            \"predicted\": None,\n",
        "            \"gold\": gold_aspects\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    # Lemmatizē visus aspektus\n",
        "    predicted_aspects = [lemmatize_phrase(asp) for asp in predicted[\"results\"]]\n",
        "    gold_aspects_lemmas = [lemmatize_phrase(asp) for asp in gold_aspects]\n",
        "\n",
        "    gold_set = set(gold_aspects_lemmas)\n",
        "    pred_set = set(predicted_aspects)\n",
        "\n",
        "\n",
        "    # TP: korekti atrastie aspekti\n",
        "    tp_aspects = gold_set & pred_set\n",
        "    TP += len(tp_aspects)\n",
        "\n",
        "    # FP: modeļa liekie aspekti\n",
        "    fp_aspects = pred_set - gold_set\n",
        "    FP += len(fp_aspects)\n",
        "    for asp in fp_aspects:\n",
        "        errors.append({\n",
        "            \"id\": sent_id,\n",
        "            \"error_type\": \"FP\",\n",
        "            \"predicted\": asp,\n",
        "            \"gold\": gold_aspects\n",
        "        })\n",
        "\n",
        "    # FN: aspekti, kurus modelis izlaida\n",
        "    fn_aspects = gold_set - pred_set\n",
        "    FN += len(fn_aspects)\n",
        "    for asp in fn_aspects:\n",
        "        errors.append({\n",
        "            \"id\": sent_id,\n",
        "            \"error_type\": \"FN\",\n",
        "            \"missed_gold_aspect\": asp,\n",
        "            \"predicted\": predicted_aspects\n",
        "        })\n",
        "\n",
        "\n",
        "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
        "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "print(\"\\nPĀRRĒĶINĀTĀS METRIKAS:\")\n",
        "print(f\"TP: {TP}\")\n",
        "print(f\"FP: {FP}\")\n",
        "print(f\"FN: {FN}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 score:  {f1:.4f}\")\n",
        "\n",
        "with open(\"adjusted_llm_errors.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(errors, f, ensure_ascii=False, indent=2)\n",
        "print(\"\\nSaglabātas kļūdas failā: adjusted_llm_errors.json\")\n"
      ],
      "metadata": {
        "id": "Qv5VKFAlwxfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MISTRAL MODEĻA ATBILŽU PAPILDUS APSTRĀDEI ASPEKTU IZGŪŠANĀ AR LEMMATIZĀCIJU"
      ],
      "metadata": {
        "id": "nYAqWQyhxmrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install stanza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xiBnmqU8KAS_",
        "outputId": "e39a19a7-bfca-410e-cfe6-8847301fa0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from stanza) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (5.29.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from stanza) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (4.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
            "Downloading stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, emoji, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stanza\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed emoji-2.14.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stanza-1.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI. GPT-4o. Uzvedne: pēc visām izskaidrotajām niansēm uzraksti pilnvērtīgu kodu, kas ļautu šīs atbildes apstrādāt. TP FP FN es ievietošu pašā kodā piem TP=476, FP=256, FN=123 https://chatgpt.com/ [izmantots 2025-05-08]\n",
        "# OpenAI. GPT-4o. Uzvedne: kā ieviest lemmatizāciju tā, lai lemmatizācija aspektiem tiktu veikta pašā pārbaudē, kur tiek skaitīti TP FP FN? https://chatgpt.com/ [izmantots 2025-05-08]\n",
        "\n",
        "import stanza\n",
        "\n",
        "stanza.download(\"lv\")\n",
        "nlp = stanza.Pipeline(\"lv\", processors=\"tokenize,pos,lemma\", tokenize_no_ssplit=True)\n",
        "def lemmatize_phrase(text):\n",
        "    doc = nlp(text.lower())\n",
        "    return \" \".join([word.lemma for sent in doc.sentences for word in sent.words])\n",
        "\n",
        "import re\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "TP = 197\n",
        "FP = 338\n",
        "FN = 307\n",
        "\n",
        "log_file = \"llm_errors_aspects.log\"\n",
        "xml_file = \"testGOLD2016_1.xml\"\n",
        "\n",
        "def get_gold_aspects(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    gold_data = {}\n",
        "    for sentence in root.iter(\"sentence\"):\n",
        "        sent_id = sentence.get(\"id\")\n",
        "        if sentence.find(\"Opinions\") is not None:\n",
        "            aspects = []\n",
        "            for op in sentence.find(\"Opinions\").findall(\"Opinion\"):\n",
        "                target = op.get(\"target\")\n",
        "                if target and target.lower() != \"null\":\n",
        "                    aspects.append(target.lower())\n",
        "            if aspects:\n",
        "                gold_data[sent_id] = aspects\n",
        "    return gold_data\n",
        "\n",
        "def extract_first_json(text):\n",
        "    matches = re.findall(r\"\\{[\\s\\S]*?\\\"results\\\"\\s*:\\s*\\[.*?\\]\\}\", text)\n",
        "    if matches:\n",
        "        try:\n",
        "            return json.loads(matches[0])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "gold_aspects_dict = get_gold_aspects(xml_file)\n",
        "errors = []\n",
        "\n",
        "with open(log_file, encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "entries = re.split(r\"\\n(?=\\w+_[^:]+:\\d+ — Extra data)\", content)\n",
        "\n",
        "for entry in entries:\n",
        "    if not entry.strip():\n",
        "        continue\n",
        "\n",
        "    match_id = re.match(r\"(\\w+_\\w+_\\d+:\\d+)\", entry)\n",
        "    if not match_id:\n",
        "        continue\n",
        "\n",
        "    sent_id = match_id.group(1)\n",
        "    gold_aspects = gold_aspects_dict.get(sent_id, [])\n",
        "\n",
        "    FN -= len(gold_aspects)\n",
        "\n",
        "    predicted = extract_first_json(entry)\n",
        "    if predicted is None or \"results\" not in predicted:\n",
        "        FN += len(gold_aspects)\n",
        "        errors.append({\n",
        "            \"id\": sent_id,\n",
        "            \"error_type\": \"NO_JSON\",\n",
        "            \"predicted\": None,\n",
        "            \"gold\": gold_aspects\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    predicted_aspects = [lemmatize_phrase(asp) for asp in predicted[\"results\"]]\n",
        "    gold_aspects_lemmas = [lemmatize_phrase(asp) for asp in gold_aspects]\n",
        "\n",
        "    gold_set = set(gold_aspects_lemmas)\n",
        "    pred_set = set(predicted_aspects)\n",
        "\n",
        "\n",
        "    # TP: korekti atrastie aspekti\n",
        "    tp_aspects = gold_set & pred_set\n",
        "    TP += len(tp_aspects)\n",
        "\n",
        "    # FP: liekie aspekti\n",
        "    fp_aspects = pred_set - gold_set\n",
        "    FP += len(fp_aspects)\n",
        "    for asp in fp_aspects:\n",
        "        errors.append({\n",
        "            \"id\": sent_id,\n",
        "            \"error_type\": \"FP\",\n",
        "            \"predicted\": asp,\n",
        "            \"gold\": gold_aspects\n",
        "        })\n",
        "\n",
        "    # FN: aspekti, ko modelis neizgūst\n",
        "    fn_aspects = gold_set - pred_set\n",
        "    FN += len(fn_aspects)\n",
        "    for asp in fn_aspects:\n",
        "        errors.append({\n",
        "            \"id\": sent_id,\n",
        "            \"error_type\": \"FN\",\n",
        "            \"missed_gold_aspect\": asp,\n",
        "            \"predicted\": predicted_aspects\n",
        "        })\n",
        "\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(\"\\nPĀRRĒĶINĀTĀS METRIKAS (Mistral):\")\n",
        "print(f\"TP: {TP}\")\n",
        "print(f\"FP: {FP}\")\n",
        "print(f\"FN: {FN}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 score:  {f1:.4f}\")\n",
        "\n",
        "with open(\"adjusted_llm_errors_mistral.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(errors, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nSaglabātas kļūdas failā: adjusted_llm_errors_mistral.json\")\n"
      ],
      "metadata": {
        "id": "fQKgUI4Uxocj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}